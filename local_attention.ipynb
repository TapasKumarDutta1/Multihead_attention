{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "local_attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMlolLbsVVT0ooqEVPP19AJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/Multihead_attention/blob/master/local_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_-Hg4tUuNbd"
      },
      "source": [
        "\r\n",
        "import torch\r\n",
        "import math\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from operator import mul\r\n",
        "from functools import reduce\r\n",
        "\r\n",
        "# constant\r\n",
        "\r\n",
        "TOKEN_SELF_ATTN_VALUE = -5e4 # carefully set for half precision to work\r\n",
        "\r\n",
        "# helper functions\r\n",
        "\r\n",
        "def default(value, d):\r\n",
        "    return d if value is None else value\r\n",
        "\r\n",
        "def to(t):\r\n",
        "    return {'device': t.device, 'dtype': t.dtype}\r\n",
        "\r\n",
        "def max_neg_value(tensor):\r\n",
        "    return -torch.finfo(tensor.dtype).max\r\n",
        "\r\n",
        "def merge_dims(ind_from, ind_to, tensor):\r\n",
        "    shape = list(tensor.shape)\r\n",
        "    arr_slice = slice(ind_from, ind_to + 1)\r\n",
        "    shape[arr_slice] = [reduce(mul, shape[arr_slice])]\r\n",
        "    return tensor.reshape(*shape)\r\n",
        "\r\n",
        "def expand_dim(t, dim, k, unsqueeze=True):\r\n",
        "    if unsqueeze:\r\n",
        "        t = t.unsqueeze(dim)\r\n",
        "    expand_shape = [-1] * len(t.shape)\r\n",
        "    expand_shape[dim] = k\r\n",
        "    return t.expand(*expand_shape)\r\n",
        "\r\n",
        "def pad_to_multiple(tensor, multiple, dim=-1, value=0):\r\n",
        "    seqlen = tensor.shape[dim]\r\n",
        "    m = seqlen / multiple\r\n",
        "    if m.is_integer():\r\n",
        "        return tensor\r\n",
        "    remainder = math.ceil(m) * multiple - seqlen\r\n",
        "    pad_offset = (0,) * (-1 - dim) * 2\r\n",
        "    return F.pad(tensor, (*pad_offset, 0, remainder), value=value)\r\n",
        "\r\n",
        "def look_around(x, backward = 1, forward = 0, pad_value = -1, dim = 2):\r\n",
        "    t = x.shape[1]\r\n",
        "    dims = (len(x.shape) - dim) * (0, 0)\r\n",
        "    padded_x = F.pad(x, (*dims, backward, forward), value= pad_value)\r\n",
        "    tensors = [padded_x[:, ind:(ind + t), ...] for ind in range(forward + backward + 1)]\r\n",
        "    return torch.cat(tensors, dim=dim)\r\n",
        "\r\n",
        "# Shaw's relative positional encoding per window\r\n",
        "\r\n",
        "def shift(x):\r\n",
        "    *_, i, j = x.shape\r\n",
        "    zero_pad = torch.zeros((*_, i, i), **to(x))\r\n",
        "    x = torch.cat([x, zero_pad], -1)\r\n",
        "    l = i + j - 1\r\n",
        "    x = x.view(*_, -1)\r\n",
        "    zero_pad = torch.zeros(*_, -x.size(-1) % l, **to(x))\r\n",
        "    shifted = torch.cat([x, zero_pad], -1).view(*_, -1, l)\r\n",
        "    return shifted[..., :i, i - 1:]\r\n",
        "\r\n",
        "class RelativePositionalEmbedding(nn.Module):\r\n",
        "    def __init__(self, dim, heads, length):\r\n",
        "        super().__init__()\r\n",
        "        self.scale = dim ** -0.5\r\n",
        "        self.weights = nn.Parameter(torch.zeros(length, heads, dim))\r\n",
        "\r\n",
        "    def forward(self, q):\r\n",
        "        emb = torch.einsum('bhnid,jhd->bhnij', q, self.weights.type(q.dtype)) * self.scale\r\n",
        "        return shift(emb)\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkDq6ql9uTVI"
      },
      "source": [
        "\r\n",
        "# main class\r\n",
        "\r\n",
        "class LocalAttention(nn.Module):\r\n",
        "    def __init__(self, window_size=32, causal = False, look_backward = 1, look_forward = None, dropout = 0., shared_qk = False, rel_pos_emb_config = None, autopad = False, exact_windowsize = False):\r\n",
        "        super().__init__()\r\n",
        "        look_forward = default(look_forward, 0 if causal else 1)\r\n",
        "        assert not (causal and look_forward > 0), 'you cannot look forward if causal'\r\n",
        "\r\n",
        "        self.window_size = window_size\r\n",
        "        self.causal = causal\r\n",
        "        self.look_backward = look_backward\r\n",
        "        self.look_forward = look_forward\r\n",
        "        self.exact_windowsize = exact_windowsize\r\n",
        "        self.autopad = autopad\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "        self.shared_qk = shared_qk\r\n",
        "\r\n",
        "        self.rel_pos = None\r\n",
        "        if rel_pos_emb_config is not None:\r\n",
        "            dim_head, heads = rel_pos_emb_config\r\n",
        "            rel_pos_length = window_size * (1 + look_forward + look_backward)\r\n",
        "            self.heads = heads\r\n",
        "            self.rel_pos = RelativePositionalEmbedding(dim_head, heads, rel_pos_length)\r\n",
        "\r\n",
        "    def forward(self, q, k, v, input_mask = None):\r\n",
        "        shape = q.shape\r\n",
        "\r\n",
        "        merge_into_batch = lambda t: t.reshape(-1, *t.shape[-2:])\r\n",
        "        q, k, v = map(merge_into_batch, (q, k, v))\r\n",
        "\r\n",
        "        if self.autopad:\r\n",
        "            orig_t = q.shape[1]\r\n",
        "            q, k, v = map(lambda t: pad_to_multiple(t, self.window_size, dim = -2), (q, k, v))\r\n",
        "\r\n",
        "        window_size, causal, look_backward, look_forward, shared_qk = self.window_size, self.causal, self.look_backward, self.look_forward, self.shared_qk\r\n",
        "        b, t, e, device, dtype = *q.shape, q.device, q.dtype\r\n",
        "        assert (t % window_size) == 0, f'sequence length {t} must be divisible by window size {window_size} for local attention'\r\n",
        "\r\n",
        "        windows = t // window_size\r\n",
        "\r\n",
        "        if shared_qk:\r\n",
        "            k = F.normalize(k, 2, dim=-1).type_as(q)\r\n",
        "\r\n",
        "        ticker = torch.arange(t, device=device, dtype=dtype)[None, :]\r\n",
        "        b_t = ticker.reshape(1, windows, window_size)\r\n",
        "\r\n",
        "        bucket_fn = lambda t: t.reshape(b, windows, window_size, -1)\r\n",
        "        bq, bk, bv = map(bucket_fn, (q, k, v))\r\n",
        "\r\n",
        "        look_around_kwargs = {'backward': look_backward, 'forward': look_forward}\r\n",
        "        bk = look_around(bk, **look_around_kwargs)\r\n",
        "        bv = look_around(bv, **look_around_kwargs)\r\n",
        "\r\n",
        "        bq_t = b_t\r\n",
        "        bq_k = look_around(b_t, **look_around_kwargs)\r\n",
        "\r\n",
        "        dots = torch.einsum('bhie,bhje->bhij', bq, bk) * (e ** -0.5)\r\n",
        "\r\n",
        "        if self.rel_pos is not None:\r\n",
        "            rel_attn = self.rel_pos(bq.view(-1, self.heads, *bq.shape[1:])).reshape_as(dots)\r\n",
        "            dots = dots + rel_attn\r\n",
        "\r\n",
        "        mask_value = max_neg_value(dots)\r\n",
        "\r\n",
        "        if shared_qk:\r\n",
        "            mask = bq_t[:, :, :, None] == bq_k[:, :, None, :]\r\n",
        "            dots.masked_fill_(mask, TOKEN_SELF_ATTN_VALUE)\r\n",
        "            del mask\r\n",
        "\r\n",
        "        if causal:\r\n",
        "            mask = bq_t[:, :, :, None] < bq_k[:, :, None, :]\r\n",
        "\r\n",
        "            if self.exact_windowsize:\r\n",
        "                max_causal_window_size = (self.window_size * self.look_backward)\r\n",
        "                mask = mask | (bq_t[:, :, :, None] > (bq_k[:, :, None, :] + max_causal_window_size))\r\n",
        "\r\n",
        "            dots.masked_fill_(mask, mask_value)\r\n",
        "            del mask\r\n",
        "\r\n",
        "        mask = bq_k[:, :, None, :] == -1\r\n",
        "        dots.masked_fill_(mask, mask_value)\r\n",
        "        del mask\r\n",
        "\r\n",
        "        if input_mask is not None:\r\n",
        "            h = b // input_mask.shape[0]\r\n",
        "            if self.autopad:\r\n",
        "                input_mask = pad_to_multiple(input_mask, window_size, dim=-1, value=False)\r\n",
        "            input_mask = input_mask.reshape(-1, windows, window_size)\r\n",
        "            mq = mk = input_mask\r\n",
        "            mk = look_around(mk, pad_value=False, **look_around_kwargs)\r\n",
        "            mask = (mq[:, :, :, None] * mk[:, :, None, :])\r\n",
        "            mask = merge_dims(0, 1, expand_dim(mask, 1, h))\r\n",
        "            dots.masked_fill_(~mask, mask_value)\r\n",
        "            del mask\r\n",
        "\r\n",
        "        attn = dots.softmax(dim=-1)\r\n",
        "        attn = self.dropout(attn)\r\n",
        "\r\n",
        "        out = torch.einsum('bhij,bhje->bhie', attn, bv)\r\n",
        "        out = out.reshape(-1, t, e)\r\n",
        "\r\n",
        "        if self.autopad:\r\n",
        "            out = out[:, :orig_t, :]\r\n",
        "\r\n",
        "        return out.reshape(*shape)\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEB-cbznubkR",
        "outputId": "521062ab-def0-468f-8544-5e2f34948e16"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4raSa_GtuoAh"
      },
      "source": [
        "import numpy as np\r\n",
        "path = F\"/content/gdrive/My Drive/check.npy\" \r\n",
        "df=np.load(path,allow_pickle=True)\r\n",
        "df=df.item()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00pAZX9ZvEOq"
      },
      "source": [
        "\r\n",
        "\r\n",
        "#shuffle samples\r\n",
        "def unison_shuffled_copies(a, b):\r\n",
        "    assert len(a) == len(b)\r\n",
        "    p = np.random.permutation(len(a))\r\n",
        "    return a[p], b[p]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#change targets\r\n",
        "def change(img):\r\n",
        "    resized = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA )\r\n",
        "    return resized\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#get train and test splits\r\n",
        "def get_trn_tst(df,tst_fold):\r\n",
        "  idx=np.asarray(df['fold'])\r\n",
        "  y=np.asarray(df['label'])\r\n",
        "  y-=1\r\n",
        "  img=np.asarray(df['image'])\r\n",
        "  img1=[]\r\n",
        "  for i in range(len(img)):\r\n",
        "        img1.append(change(img[i]))\r\n",
        "  img1=np.asarray(img1)\r\n",
        "  del([img])\r\n",
        "  gc.collect()\r\n",
        "  trn_y=np.asarray(y[(idx!=tst_fold)])\r\n",
        "  trn_img=np.asarray(img1[(idx!=tst_fold)])\r\n",
        "  tst_y=np.asarray(y[(idx==tst_fold)])\r\n",
        "  tst_img=img1[idx==tst_fold]\r\n",
        "  trn_img=np.repeat(trn_img.reshape((trn_img.shape[0],224,224,1)),3,axis=3)\r\n",
        "  tst_img=np.repeat(tst_img.reshape((tst_img.shape[0],224,224,1)),3,axis=3)\r\n",
        "  return (trn_img.copy(),trn_y.copy()),(tst_img.copy(),tst_y.copy())\r\n",
        "\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmLF4Q4bvG2e"
      },
      "source": [
        "\r\n",
        "from __future__ import print_function\r\n",
        "from __future__ import division\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.nn import Parameter\r\n",
        "import math\r\n",
        "from torch.nn.parameter import Parameter\r\n",
        "import torchvision.models as models\r\n",
        "from torch.autograd import Variable\r\n",
        "class LyftMultiModel(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.backbone=torch.nn.Conv2d(3, 18, kernel_size = 3, stride = 1, padding = 1)\r\n",
        "        self.att=LocalAttention()\r\n",
        "        self.lnr1=nn.Linear(903168,1)\r\n",
        "    def forward(self, x):\r\n",
        "            x = self.backbone(x)\r\n",
        "            x=self.att(x,x,x)\r\n",
        "            print(x.shape)\r\n",
        "            x=x.view(3,-1)\r\n",
        "            x=self.lnr1(x)\r\n",
        "            return x\r\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN8CQUWiwfx2"
      },
      "source": [
        "mod=LyftMultiModel().double()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7xZT3FQw6wA",
        "outputId": "3fd7c6c3-ca96-4a34-9637-9f8b612aa64f"
      },
      "source": [
        "a=mod(torch.from_numpy(np.repeat(df['image'][:3,:,:].reshape((3,1,224,224)),3,1)).double())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 18, 224, 224])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NciLjsCOyRo2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}