{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multihead_final_deeper_diferent_5_multiple_densenet_concatenate_all_SpatialDropout1D.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/Multihead_attention/blob/master/multihead_final_deeper_diferent_5_multiple_densenet_concatenate_all_SpatialDropout1D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "47845a64-23a9-43f7-8dd7-49ce84aaedc3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import h5py\n",
        "from tensorflow.keras.optimizers import *\n",
        "import cv2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import glob, os\n",
        "from matplotlib import pyplot as plt\n",
        "import h5py\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import gc\n",
        "from tensorflow.keras.applications import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = F\"/content/gdrive/My Drive/check.npy\" \n",
        "df=np.load(path,allow_pickle=True)\n",
        "df=df.item()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#shuffle samples\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "\n",
        "\n",
        "#change targets\n",
        "def change(img):\n",
        "    resized = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA )\n",
        "    return resized\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#get train and test splits\n",
        "def get_trn_tst(df,tst_fold):\n",
        "  idx=np.asarray(df['fold'])\n",
        "  y=np.asarray(df['label'])\n",
        "  y-=1\n",
        "  img=np.asarray(df['image'])\n",
        "  img1=[]\n",
        "  for i in range(len(img)):\n",
        "        img1.append(change(img[i]))\n",
        "  img1=np.asarray(img1)\n",
        "  del([img])\n",
        "  gc.collect()\n",
        "  trn_y=np.asarray(y[(idx!=tst_fold)])\n",
        "  trn_img=np.asarray(img1[(idx!=tst_fold)])\n",
        "  tst_y=np.asarray(y[(idx==tst_fold)])\n",
        "  tst_img=img1[idx==tst_fold]\n",
        "  trn_img=np.repeat(trn_img.reshape((trn_img.shape[0],224,224,1)),3,axis=3)\n",
        "  tst_img=np.repeat(tst_img.reshape((tst_img.shape[0],224,224,1)),3,axis=3)\n",
        "  return (trn_img.copy(),trn_y.copy()),(tst_img.copy(),tst_y.copy())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "class LayerNormalization(Layer):\n",
        "    def __init__(self, eps=1e-6, **kwargs):\n",
        "        self.eps = eps\n",
        "        super(LayerNormalization, self).__init__(**kwargs)\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
        "                                     initializer=Ones(), trainable=True)\n",
        "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
        "                                    initializer=Zeros(), trainable=True)\n",
        "        super(LayerNormalization, self).build(input_shape)\n",
        "    def call(self, x):\n",
        "        mean = K.mean(x, axis=-1, keepdims=True)\n",
        "        std = K.std(x, axis=-1, keepdims=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "class abc(keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 head_num,\n",
        "                 q_k,\n",
        "                 activation='relu',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_normal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 history_only=False,\n",
        "                 **kwargs):\n",
        "        self.q_k=q_k\n",
        "        self.supports_masking = True\n",
        "        self.head_num = head_num\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
        "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
        "        self.kernel_constraint = keras.constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = keras.constraints.get(bias_constraint)\n",
        "        self.history_only = history_only\n",
        "\n",
        "        self.Wq = self.Wk = self.Wv = self.Wo = None\n",
        "        self.bq = self.bk = self.bv = self.bo = None\n",
        "\n",
        "        self.intensity = self.attention = None\n",
        "        super(abc, self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'head_num': self.head_num,\n",
        "            'activation': keras.activations.serialize(self.activation),\n",
        "            'use_bias': self.use_bias,\n",
        "            'kernel_initializer': keras.initializers.serialize(self.kernel_initializer),\n",
        "            'bias_initializer': keras.initializers.serialize(self.bias_initializer),\n",
        "            'kernel_regularizer': keras.regularizers.serialize(self.kernel_regularizer),\n",
        "            'bias_regularizer': keras.regularizers.serialize(self.bias_regularizer),\n",
        "            'kernel_constraint': keras.constraints.serialize(self.kernel_constraint),\n",
        "            'bias_constraint': keras.constraints.serialize(self.bias_constraint),\n",
        "            'history_only': self.history_only,\n",
        "        }\n",
        "        base_config = super(abc, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if isinstance(input_shape, list):\n",
        "            q, k, v = input_shape\n",
        "            return q[:-1] + (v[-1],)\n",
        "        return input_shape\n",
        "\n",
        "    def compute_mask(self, inputs, input_mask=None):\n",
        "        if isinstance(input_mask, list):\n",
        "            return input_mask[0]\n",
        "        return input_mask\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.layer_norm = LayerNormalization()\n",
        "        if isinstance(input_shape, list):\n",
        "            q, k, v = input_shape\n",
        "        else:\n",
        "            q = k = v = input_shape\n",
        "        feature_dim = int(v[-1])\n",
        "        if feature_dim % self.head_num != 0:\n",
        "            raise IndexError('Invalid head number %d with the given input dim %d' % (self.head_num, feature_dim))\n",
        "        self.Wq = self.add_weight(\n",
        "            shape=(int(q[-1]), self.q_k),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            name='%s_Wq' % self.name,\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.bq = self.add_weight(\n",
        "                shape=(self.q_k,),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name='%s_bq' % self.name,\n",
        "            )\n",
        "        self.Wk = self.add_weight(\n",
        "            shape=(int(k[-1]), self.q_k),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            name='%s_Wk' % self.name,\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.bk = self.add_weight(\n",
        "                shape=(self.q_k,),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name='%s_bk' % self.name,\n",
        "            )\n",
        "        self.Wv = self.add_weight(\n",
        "            shape=(int(v[-1]), feature_dim),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            name='%s_Wv' % self.name,\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.bv = self.add_weight(\n",
        "                shape=(feature_dim,),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name='%s_bv' % self.name,\n",
        "            )\n",
        "        self.Wo = self.add_weight(\n",
        "            shape=(feature_dim, feature_dim),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            name='%s_Wo' % self.name,\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.bo = self.add_weight(\n",
        "                shape=(feature_dim,),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name='%s_bo' % self.name,\n",
        "            )\n",
        "        super(abc, self).build(input_shape)\n",
        "\n",
        "    @staticmethod\n",
        "    def _reshape_to_batches(x, head_num):\n",
        "        input_shape = K.shape(x)\n",
        "        batch_size, seq_len, feature_dim = input_shape[0], input_shape[1], input_shape[2]\n",
        "        head_dim = feature_dim // head_num\n",
        "        x = K.reshape(x, (batch_size, seq_len, head_num, head_dim))\n",
        "        x = K.permute_dimensions(x, [0, 2, 1, 3])\n",
        "        return K.reshape(x, (batch_size * head_num, seq_len, head_dim))\n",
        "\n",
        "    @staticmethod\n",
        "    def _reshape_attention_from_batches(x, head_num):\n",
        "        input_shape = K.shape(x)\n",
        "        batch_size, seq_len, feature_dim = input_shape[0], input_shape[1], input_shape[2]\n",
        "        x = K.reshape(x, (batch_size // head_num, head_num, seq_len, feature_dim))\n",
        "        return K.permute_dimensions(x, [0, 2, 1, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def _reshape_from_batches(x, head_num):\n",
        "        input_shape = K.shape(x)\n",
        "        batch_size, seq_len, feature_dim = input_shape[0], input_shape[1], input_shape[2]\n",
        "        x = K.reshape(x, (batch_size // head_num, head_num, seq_len, feature_dim))\n",
        "        x = K.permute_dimensions(x, [0, 2, 1, 3])\n",
        "        return K.reshape(x, (batch_size // head_num, seq_len, feature_dim * head_num))\n",
        "\n",
        "    @staticmethod\n",
        "    def _reshape_mask(mask, head_num):\n",
        "        if mask is None:\n",
        "            return mask\n",
        "        seq_len = K.shape(mask)[1]\n",
        "        mask = K.expand_dims(mask, axis=1)\n",
        "        mask = K.tile(mask, [1, head_num, 1])\n",
        "        return K.reshape(mask, (-1, seq_len))\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if isinstance(inputs, list):\n",
        "            q, k, v = inputs\n",
        "        else:\n",
        "            q = k = v = inputs\n",
        "        if isinstance(mask, list):\n",
        "            q_mask, k_mask, v_mask = mask\n",
        "        else:\n",
        "            q_mask = k_mask = v_mask = mask\n",
        "        q = K.dot(q, self.Wq)\n",
        "        k = K.dot(k, self.Wk)\n",
        "        v = K.dot(v, self.Wv)\n",
        "        if self.use_bias:\n",
        "            q += self.bq\n",
        "            k += self.bk\n",
        "            v += self.bv\n",
        "        if self.activation is not None:\n",
        "            q = self.activation(q)\n",
        "            k = self.activation(k)\n",
        "            v = self.activation(v)\n",
        "        def scaled_dot_product_attention(inputs):\n",
        "          query, key, value = inputs\n",
        "          feature_dim = K.shape(query)[-1]\n",
        "          e = K.batch_dot(query, key, axes=2) / K.sqrt(K.cast(self.q_k, dtype=K.floatx()))\n",
        "          intensity = e\n",
        "          e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n",
        "          attention = e / K.sum(e, axis=-1, keepdims=True)\n",
        "          attention = Dropout(0.2)(attention)\n",
        "          v = K.batch_dot(attention, value)\n",
        "          return v,intensity,attention\n",
        "       \n",
        "       \n",
        "        y,intensity,attention = scaled_dot_product_attention(\n",
        "            inputs=[\n",
        "                self._reshape_to_batches(q, self.head_num),\n",
        "                self._reshape_to_batches(k, self.head_num),\n",
        "                self._reshape_to_batches(v, self.head_num),\n",
        "            ]\n",
        "        )\n",
        "        self.intensity = self._reshape_attention_from_batches(intensity, self.head_num)\n",
        "        self.attention = self._reshape_attention_from_batches(attention, self.head_num)\n",
        "        y = self._reshape_from_batches(y, self.head_num)\n",
        "        y = K.dot(y, self.Wo)\n",
        "        if self.use_bias:\n",
        "            y += self.bo\n",
        "        if self.activation is not None:\n",
        "            y = self.activation(y)\n",
        "        return y\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJem4-mp8otc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras \n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.applications import *\n",
        "def load_model():   \n",
        "  \n",
        "  K.clear_session() \n",
        "  mod=densenet.DenseNet121(include_top=True, weights='imagenet')\n",
        "\n",
        "  a = mod.get_layer('pool4_pool').output\n",
        "  a = Reshape((7*7,512))(a)\n",
        "  a = SpatialDropout1D(0.001)(a)\n",
        "  a = abc(head_num=1,q_k=512)(a)\n",
        "  a = LayerNormalization()(a)\n",
        "  a = Reshape((7*7*512,))(a)\n",
        "\n",
        "  b = mod.get_layer('pool1').output\n",
        "  b = Reshape((56*56,64))(b)\n",
        "  b = SpatialDropout1D(0.001)(b)\n",
        "  b = abc(head_num=1,q_k=64)(b)\n",
        "  b = LayerNormalization()(b)\n",
        "  b = Reshape((56*56*64,))(b)\n",
        "  \n",
        "  c = mod.get_layer('pool2_pool').output\n",
        "  c = Reshape((28*28,128))(c)\n",
        "  c = SpatialDropout1D(0.001)(c)\n",
        "  c = abc(head_num=1,q_k=128)(c)\n",
        "  c = LayerNormalization()(c)\n",
        "  c = Reshape((28*28*128,))(c)\n",
        "  \n",
        "  d = mod.get_layer('pool3_pool').output\n",
        "  d = Reshape((14*14,256))(d)\n",
        "  d = SpatialDropout1D(0.001)(d)\n",
        "  d = abc(head_num=1,q_k=256)(d)\n",
        "  d = LayerNormalization()(d)\n",
        "  d = Reshape((14*14*256,))(d)\n",
        "  \n",
        "  conc=Concatenate()([a,b,c,d])\n",
        "  x = Dense(3, activation=\"softmax\")(conc) \n",
        "  mod=Model(inputs=mod.input,outputs=x)\n",
        "  return mod"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4aedc620-5626-4703-aab7-4836eae54cc2"
      },
      "source": [
        "mod=load_model()\n",
        "mod.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 230, 230, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1/conv (Conv2D)             (None, 112, 112, 64) 9408        zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv1/bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1/conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1/relu (Activation)         (None, 112, 112, 64) 0           conv1/bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 114, 114, 64) 0           conv1/relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 56, 56, 64)   0           zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 64)   256         pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_relu (Activation (None, 56, 56, 64)   0           conv2_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 128)  8192        conv2_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 128)  0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_concat (Concatenat (None, 56, 56, 96)   0           pool1[0][0]                      \n",
            "                                                                 conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_bn (BatchNormali (None, 56, 56, 96)   384         conv2_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_relu (Activation (None, 56, 56, 96)   0           conv2_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 128)  12288       conv2_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 128)  0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_concat (Concatenat (None, 56, 56, 128)  0           conv2_block1_concat[0][0]        \n",
            "                                                                 conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_relu (Activation (None, 56, 56, 128)  0           conv2_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 128)  16384       conv2_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 128)  0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_concat (Concatenat (None, 56, 56, 160)  0           conv2_block2_concat[0][0]        \n",
            "                                                                 conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_bn (BatchNormali (None, 56, 56, 160)  640         conv2_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_relu (Activation (None, 56, 56, 160)  0           conv2_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_conv (Conv2D)    (None, 56, 56, 128)  20480       conv2_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_relu (Activation (None, 56, 56, 128)  0           conv2_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_concat (Concatenat (None, 56, 56, 192)  0           conv2_block3_concat[0][0]        \n",
            "                                                                 conv2_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_bn (BatchNormali (None, 56, 56, 192)  768         conv2_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_relu (Activation (None, 56, 56, 192)  0           conv2_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_conv (Conv2D)    (None, 56, 56, 128)  24576       conv2_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_relu (Activation (None, 56, 56, 128)  0           conv2_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_concat (Concatenat (None, 56, 56, 224)  0           conv2_block4_concat[0][0]        \n",
            "                                                                 conv2_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_bn (BatchNormali (None, 56, 56, 224)  896         conv2_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_relu (Activation (None, 56, 56, 224)  0           conv2_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_conv (Conv2D)    (None, 56, 56, 128)  28672       conv2_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_relu (Activation (None, 56, 56, 128)  0           conv2_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_concat (Concatenat (None, 56, 56, 256)  0           conv2_block5_concat[0][0]        \n",
            "                                                                 conv2_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_bn (BatchNormalization)   (None, 56, 56, 256)  1024        conv2_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_relu (Activation)         (None, 56, 56, 256)  0           pool2_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool2_conv (Conv2D)             (None, 56, 56, 128)  32768       pool2_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool2_pool (AveragePooling2D)   (None, 28, 28, 128)  0           pool2_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 128)  512         pool2_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_relu (Activation (None, 28, 28, 128)  0           conv3_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  16384       conv3_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_concat (Concatenat (None, 28, 28, 160)  0           pool2_pool[0][0]                 \n",
            "                                                                 conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_bn (BatchNormali (None, 28, 28, 160)  640         conv3_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_relu (Activation (None, 28, 28, 160)  0           conv3_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  20480       conv3_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_concat (Concatenat (None, 28, 28, 192)  0           conv3_block1_concat[0][0]        \n",
            "                                                                 conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_bn (BatchNormali (None, 28, 28, 192)  768         conv3_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_relu (Activation (None, 28, 28, 192)  0           conv3_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  24576       conv3_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_concat (Concatenat (None, 28, 28, 224)  0           conv3_block2_concat[0][0]        \n",
            "                                                                 conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_bn (BatchNormali (None, 28, 28, 224)  896         conv3_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_relu (Activation (None, 28, 28, 224)  0           conv3_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  28672       conv3_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_concat (Concatenat (None, 28, 28, 256)  0           conv3_block3_concat[0][0]        \n",
            "                                                                 conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_relu (Activation (None, 28, 28, 256)  0           conv3_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_relu (Activation (None, 28, 28, 128)  0           conv3_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_concat (Concatenat (None, 28, 28, 288)  0           conv3_block4_concat[0][0]        \n",
            "                                                                 conv3_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_bn (BatchNormali (None, 28, 28, 288)  1152        conv3_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_relu (Activation (None, 28, 28, 288)  0           conv3_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_conv (Conv2D)    (None, 28, 28, 128)  36864       conv3_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_relu (Activation (None, 28, 28, 128)  0           conv3_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_concat (Concatenat (None, 28, 28, 320)  0           conv3_block5_concat[0][0]        \n",
            "                                                                 conv3_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_bn (BatchNormali (None, 28, 28, 320)  1280        conv3_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_relu (Activation (None, 28, 28, 320)  0           conv3_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_conv (Conv2D)    (None, 28, 28, 128)  40960       conv3_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_relu (Activation (None, 28, 28, 128)  0           conv3_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_concat (Concatenat (None, 28, 28, 352)  0           conv3_block6_concat[0][0]        \n",
            "                                                                 conv3_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_bn (BatchNormali (None, 28, 28, 352)  1408        conv3_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_relu (Activation (None, 28, 28, 352)  0           conv3_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_conv (Conv2D)    (None, 28, 28, 128)  45056       conv3_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_relu (Activation (None, 28, 28, 128)  0           conv3_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_concat (Concatenat (None, 28, 28, 384)  0           conv3_block7_concat[0][0]        \n",
            "                                                                 conv3_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_bn (BatchNormali (None, 28, 28, 384)  1536        conv3_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_relu (Activation (None, 28, 28, 384)  0           conv3_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_conv (Conv2D)    (None, 28, 28, 128)  49152       conv3_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_relu (Activation (None, 28, 28, 128)  0           conv3_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_concat (Concatenat (None, 28, 28, 416)  0           conv3_block8_concat[0][0]        \n",
            "                                                                 conv3_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_bn (BatchNormal (None, 28, 28, 416)  1664        conv3_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_relu (Activatio (None, 28, 28, 416)  0           conv3_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_conv (Conv2D)   (None, 28, 28, 128)  53248       conv3_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_concat (Concatena (None, 28, 28, 448)  0           conv3_block9_concat[0][0]        \n",
            "                                                                 conv3_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_bn (BatchNormal (None, 28, 28, 448)  1792        conv3_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_relu (Activatio (None, 28, 28, 448)  0           conv3_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_conv (Conv2D)   (None, 28, 28, 128)  57344       conv3_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_concat (Concatena (None, 28, 28, 480)  0           conv3_block10_concat[0][0]       \n",
            "                                                                 conv3_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_bn (BatchNormal (None, 28, 28, 480)  1920        conv3_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_relu (Activatio (None, 28, 28, 480)  0           conv3_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_conv (Conv2D)   (None, 28, 28, 128)  61440       conv3_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_concat (Concatena (None, 28, 28, 512)  0           conv3_block11_concat[0][0]       \n",
            "                                                                 conv3_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_bn (BatchNormalization)   (None, 28, 28, 512)  2048        conv3_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_relu (Activation)         (None, 28, 28, 512)  0           pool3_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool3_conv (Conv2D)             (None, 28, 28, 256)  131072      pool3_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool3_pool (AveragePooling2D)   (None, 14, 14, 256)  0           pool3_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 256)  1024        pool3_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_relu (Activation (None, 14, 14, 256)  0           conv4_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 128)  32768       conv4_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 128)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_concat (Concatenat (None, 14, 14, 288)  0           pool3_pool[0][0]                 \n",
            "                                                                 conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_bn (BatchNormali (None, 14, 14, 288)  1152        conv4_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_relu (Activation (None, 14, 14, 288)  0           conv4_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 128)  36864       conv4_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 128)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_concat (Concatenat (None, 14, 14, 320)  0           conv4_block1_concat[0][0]        \n",
            "                                                                 conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_bn (BatchNormali (None, 14, 14, 320)  1280        conv4_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_relu (Activation (None, 14, 14, 320)  0           conv4_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 128)  40960       conv4_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 128)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_concat (Concatenat (None, 14, 14, 352)  0           conv4_block2_concat[0][0]        \n",
            "                                                                 conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_bn (BatchNormali (None, 14, 14, 352)  1408        conv4_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_relu (Activation (None, 14, 14, 352)  0           conv4_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 128)  45056       conv4_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 128)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_concat (Concatenat (None, 14, 14, 384)  0           conv4_block3_concat[0][0]        \n",
            "                                                                 conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_bn (BatchNormali (None, 14, 14, 384)  1536        conv4_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_relu (Activation (None, 14, 14, 384)  0           conv4_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 128)  49152       conv4_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 128)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_concat (Concatenat (None, 14, 14, 416)  0           conv4_block4_concat[0][0]        \n",
            "                                                                 conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_bn (BatchNormali (None, 14, 14, 416)  1664        conv4_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_relu (Activation (None, 14, 14, 416)  0           conv4_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 128)  53248       conv4_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 128)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_concat (Concatenat (None, 14, 14, 448)  0           conv4_block5_concat[0][0]        \n",
            "                                                                 conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_bn (BatchNormali (None, 14, 14, 448)  1792        conv4_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_relu (Activation (None, 14, 14, 448)  0           conv4_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_conv (Conv2D)    (None, 14, 14, 128)  57344       conv4_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_relu (Activation (None, 14, 14, 128)  0           conv4_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_concat (Concatenat (None, 14, 14, 480)  0           conv4_block6_concat[0][0]        \n",
            "                                                                 conv4_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_bn (BatchNormali (None, 14, 14, 480)  1920        conv4_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_relu (Activation (None, 14, 14, 480)  0           conv4_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_conv (Conv2D)    (None, 14, 14, 128)  61440       conv4_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_relu (Activation (None, 14, 14, 128)  0           conv4_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_concat (Concatenat (None, 14, 14, 512)  0           conv4_block7_concat[0][0]        \n",
            "                                                                 conv4_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_relu (Activation (None, 14, 14, 512)  0           conv4_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_conv (Conv2D)    (None, 14, 14, 128)  65536       conv4_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_relu (Activation (None, 14, 14, 128)  0           conv4_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_concat (Concatenat (None, 14, 14, 544)  0           conv4_block8_concat[0][0]        \n",
            "                                                                 conv4_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_bn (BatchNormal (None, 14, 14, 544)  2176        conv4_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_relu (Activatio (None, 14, 14, 544)  0           conv4_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_conv (Conv2D)   (None, 14, 14, 128)  69632       conv4_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_concat (Concatena (None, 14, 14, 576)  0           conv4_block9_concat[0][0]        \n",
            "                                                                 conv4_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_bn (BatchNormal (None, 14, 14, 576)  2304        conv4_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_relu (Activatio (None, 14, 14, 576)  0           conv4_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_conv (Conv2D)   (None, 14, 14, 128)  73728       conv4_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_concat (Concatena (None, 14, 14, 608)  0           conv4_block10_concat[0][0]       \n",
            "                                                                 conv4_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_bn (BatchNormal (None, 14, 14, 608)  2432        conv4_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_relu (Activatio (None, 14, 14, 608)  0           conv4_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_conv (Conv2D)   (None, 14, 14, 128)  77824       conv4_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_concat (Concatena (None, 14, 14, 640)  0           conv4_block11_concat[0][0]       \n",
            "                                                                 conv4_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_bn (BatchNormal (None, 14, 14, 640)  2560        conv4_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_relu (Activatio (None, 14, 14, 640)  0           conv4_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_conv (Conv2D)   (None, 14, 14, 128)  81920       conv4_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_concat (Concatena (None, 14, 14, 672)  0           conv4_block12_concat[0][0]       \n",
            "                                                                 conv4_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_bn (BatchNormal (None, 14, 14, 672)  2688        conv4_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_relu (Activatio (None, 14, 14, 672)  0           conv4_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_conv (Conv2D)   (None, 14, 14, 128)  86016       conv4_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_concat (Concatena (None, 14, 14, 704)  0           conv4_block13_concat[0][0]       \n",
            "                                                                 conv4_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_bn (BatchNormal (None, 14, 14, 704)  2816        conv4_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_relu (Activatio (None, 14, 14, 704)  0           conv4_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_conv (Conv2D)   (None, 14, 14, 128)  90112       conv4_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_concat (Concatena (None, 14, 14, 736)  0           conv4_block14_concat[0][0]       \n",
            "                                                                 conv4_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_bn (BatchNormal (None, 14, 14, 736)  2944        conv4_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_relu (Activatio (None, 14, 14, 736)  0           conv4_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_conv (Conv2D)   (None, 14, 14, 128)  94208       conv4_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_concat (Concatena (None, 14, 14, 768)  0           conv4_block15_concat[0][0]       \n",
            "                                                                 conv4_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_bn (BatchNormal (None, 14, 14, 768)  3072        conv4_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_relu (Activatio (None, 14, 14, 768)  0           conv4_block17_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_conv (Conv2D)   (None, 14, 14, 128)  98304       conv4_block17_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block17_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block17_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block17_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_concat (Concatena (None, 14, 14, 800)  0           conv4_block16_concat[0][0]       \n",
            "                                                                 conv4_block17_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_bn (BatchNormal (None, 14, 14, 800)  3200        conv4_block17_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_relu (Activatio (None, 14, 14, 800)  0           conv4_block18_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_conv (Conv2D)   (None, 14, 14, 128)  102400      conv4_block18_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block18_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block18_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block18_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_concat (Concatena (None, 14, 14, 832)  0           conv4_block17_concat[0][0]       \n",
            "                                                                 conv4_block18_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_bn (BatchNormal (None, 14, 14, 832)  3328        conv4_block18_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_relu (Activatio (None, 14, 14, 832)  0           conv4_block19_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_conv (Conv2D)   (None, 14, 14, 128)  106496      conv4_block19_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block19_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block19_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block19_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_concat (Concatena (None, 14, 14, 864)  0           conv4_block18_concat[0][0]       \n",
            "                                                                 conv4_block19_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_bn (BatchNormal (None, 14, 14, 864)  3456        conv4_block19_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_relu (Activatio (None, 14, 14, 864)  0           conv4_block20_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_conv (Conv2D)   (None, 14, 14, 128)  110592      conv4_block20_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block20_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block20_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block20_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_concat (Concatena (None, 14, 14, 896)  0           conv4_block19_concat[0][0]       \n",
            "                                                                 conv4_block20_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_bn (BatchNormal (None, 14, 14, 896)  3584        conv4_block20_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_relu (Activatio (None, 14, 14, 896)  0           conv4_block21_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_conv (Conv2D)   (None, 14, 14, 128)  114688      conv4_block21_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block21_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block21_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block21_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_concat (Concatena (None, 14, 14, 928)  0           conv4_block20_concat[0][0]       \n",
            "                                                                 conv4_block21_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_bn (BatchNormal (None, 14, 14, 928)  3712        conv4_block21_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_relu (Activatio (None, 14, 14, 928)  0           conv4_block22_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_conv (Conv2D)   (None, 14, 14, 128)  118784      conv4_block22_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block22_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block22_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block22_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_concat (Concatena (None, 14, 14, 960)  0           conv4_block21_concat[0][0]       \n",
            "                                                                 conv4_block22_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_bn (BatchNormal (None, 14, 14, 960)  3840        conv4_block22_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_relu (Activatio (None, 14, 14, 960)  0           conv4_block23_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_conv (Conv2D)   (None, 14, 14, 128)  122880      conv4_block23_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block23_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block23_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block23_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_concat (Concatena (None, 14, 14, 992)  0           conv4_block22_concat[0][0]       \n",
            "                                                                 conv4_block23_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_bn (BatchNormal (None, 14, 14, 992)  3968        conv4_block23_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_relu (Activatio (None, 14, 14, 992)  0           conv4_block24_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_conv (Conv2D)   (None, 14, 14, 128)  126976      conv4_block24_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block24_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block24_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block24_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_concat (Concatena (None, 14, 14, 1024) 0           conv4_block23_concat[0][0]       \n",
            "                                                                 conv4_block24_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_bn (BatchNormalization)   (None, 14, 14, 1024) 4096        conv4_block24_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_relu (Activation)         (None, 14, 14, 1024) 0           pool4_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool4_conv (Conv2D)             (None, 14, 14, 512)  524288      pool4_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool4_pool (AveragePooling2D)   (None, 7, 7, 512)    0           pool4_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 49, 512)      0           pool4_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 3136, 64)     0           pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 784, 128)     0           pool2_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_6 (Reshape)             (None, 196, 256)     0           pool3_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d (SpatialDropo (None, 49, 512)      0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_1 (SpatialDro (None, 3136, 64)     0           reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_2 (SpatialDro (None, 784, 128)     0           reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_3 (SpatialDro (None, 196, 256)     0           reshape_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "abc (abc)                       (None, None, 512)    1050624     spatial_dropout1d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "abc_1 (abc)                     (None, None, 64)     16640       spatial_dropout1d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "abc_2 (abc)                     (None, None, 128)    66048       spatial_dropout1d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "abc_3 (abc)                     (None, None, 256)    263168      spatial_dropout1d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization (LayerNorma (None, None, 512)    1024        abc[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_1 (LayerNor (None, None, 64)     128         abc_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_2 (LayerNor (None, None, 128)    256         abc_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_3 (LayerNor (None, None, 256)    512         abc_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 25088)        0           layer_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 200704)       0           layer_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_5 (Reshape)             (None, 100352)       0           layer_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_7 (Reshape)             (None, 50176)        0           layer_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 376320)       0           reshape_1[0][0]                  \n",
            "                                                                 reshape_3[0][0]                  \n",
            "                                                                 reshape_5[0][0]                  \n",
            "                                                                 reshape_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 3)            1128963     concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 7,374,531\n",
            "Trainable params: 7,321,091\n",
            "Non-trainable params: 53,440\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq6gnpm4CjDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3a1aaa8e-7a26-4ecd-f4d0-4cb9d716cdfa"
      },
      "source": [
        "\n",
        "\n",
        "import keras\n",
        "import pandas as pd\n",
        "from keras.applications.nasnet import NASNetMobile, preprocess_input\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "def rotate_image(image, angle):\n",
        "  image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "  result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
        "  return result\n",
        "def Hflip( images):\n",
        "\t\tseq = iaa.Sequential([iaa.Fliplr(1.0)])\n",
        "\t\treturn seq.augment_images(images)\n",
        "def Vflip( images):\n",
        "\t\tseq = iaa.Sequential([iaa.Flipud(1.0)])\n",
        "\t\treturn seq.augment_images(images)\n",
        "def noise(images):\n",
        "    ls=[]\n",
        "    for i in images:\n",
        "        x = np.random.normal(loc=0, scale=0.05, size=(299,299,3))\n",
        "        ls.append(i+x)\n",
        "    return ls\n",
        "def rotate(images):\n",
        "    ls=[]\n",
        "    for angle in range(-15,20,5):\n",
        "        for image in images:\n",
        "            ls.append(rotate_image(image,angle))\n",
        "    return ls\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "  def __init__(self, images, labels, batch_size=64, image_dimensions = (96 ,96 ,3), shuffle=False, augment=False):\n",
        "    self.labels       = labels              # array of labels\n",
        "    self.images = images        # array of image paths\n",
        "    self.batch_size   = batch_size          # batch size\n",
        "    self.on_epoch_end()\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.floor(self.labels.shape[0] / self.batch_size))\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.indexes = np.arange(self.labels.shape[0])\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\t\t# selects indices of data for next batch\n",
        "    indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "    # select data and load images\n",
        "    labels = self.labels.loc[indexes]\n",
        "    img = [self.images[k].astype(np.float32) for k in indexes]\n",
        "    imgH=Hflip(img)\n",
        "    imgV=Vflip(img)\n",
        "    imgR=rotate(img)\n",
        "    images=[]\n",
        "    images.extend(imgH)\n",
        "    images.extend(imgV)\n",
        "    images.extend(imgR)\n",
        "    lbl=labels.copy()\n",
        "    labels=pd.DataFrame()\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    #images = np.array([preprocess_input(img) for img in images])\n",
        "    return np.asarray(images), labels.values\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVk9YO8elt-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "best_accuracy_last={}\n",
        "final_accuracy_last={}\n",
        "history_last={}\n",
        "answers_last={}\n",
        "predictions_last={}\n",
        "predictions_last_best={}\n",
        "times_last={}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykeXpxh_lu8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8433f791-9948-46fb-89df-b15246f90c79"
      },
      "source": [
        "  def upd(dk,data):\n",
        "    if dk==0:\n",
        "        dk=data\n",
        "    else:\n",
        "        for ky in data.keys():\n",
        "            dk[ky].extend(data[ky])\n",
        "    return dk\n",
        "  index=2\n",
        "  epoch=50\n",
        "  pre_acc=0\n",
        "  best=0\n",
        "  fold='fold_'+str(index)\n",
        "  trn,tst=get_trn_tst(df,index)\n",
        "  history_last[fold]=0\n",
        "\n",
        "\n",
        "\n",
        "  plt.imshow(trn[0][0])\n",
        "  plt.show()\n",
        "  plt.imshow(tst[0][0])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  trn_x,trn_y=unison_shuffled_copies(trn[0],trn[1])\n",
        "  tst_x,tst_y=unison_shuffled_copies(tst[0],tst[1])\n",
        "\n",
        "\n",
        "\n",
        "  model=load_model()\n",
        "\n",
        "\n",
        "  \n",
        "  #compiling the model\n",
        "  model.compile(optimizer=Adam(1e-4,decay=6e-4), \n",
        "                     loss='categorical_crossentropy', \n",
        "                     metrics=['accuracy'])\n",
        "  train_data = DataGenerator(trn_x,pd.get_dummies(trn_y), batch_size=4, augment=True)\n",
        "  ln=len(trn_y)\n",
        "  # del([trn_x,trn_y,trn,tst])\n",
        "  # gc.collect()\n",
        "  #fitting the model\n",
        "  #timing\n",
        "  start=time.time()\n",
        "  hist=model.fit_generator(train_data,epochs=50,steps_per_epoch=ln//4,validation_data=(tst_x,pd.get_dummies(tst_y).values))\n",
        "  history_last[fold]=upd(history_last[fold],hist.history)\n",
        "\n",
        "  end=time.time()\n",
        "  times_last[fold]=end-start\n",
        "\n",
        "  #getting the prediction \n",
        "  pre=model.predict(tst_x)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  #select the maximum position\n",
        "  pre=np.argmax(pre,1)\n",
        "  predictions_last[fold]=pre\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  #getting the accuracy\n",
        "  new_acc=accuracy_score(pre,tst_y)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  #storing the predictions\n",
        "  final_accuracy_last[fold]=new_acc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #storing the answers\n",
        "  answers_last[fold]=tst_y\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  #freeing memory\n",
        "  del([tst_x,tst_y])\n",
        "  gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "596/596 [==============================] - 283s 474ms/step - loss: 2.7999 - accuracy: 0.7273 - val_loss: 0.3116 - val_accuracy: 0.9337\n",
            "596/596 [==============================] - 283s 474ms/step - loss: 2.7999 - accuracy: 0.7273 - val_loss: 0.3116 - val_accuracy: 0.9337\n",
            "Epoch 2/50\n",
            "Epoch 2/50\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 0.3973 - accuracy: 0.9271 - val_loss: 0.3704 - val_accuracy: 0.9308\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 0.3973 - accuracy: 0.9271 - val_loss: 0.3704 - val_accuracy: 0.9308\n",
            "Epoch 3/50\n",
            "Epoch 3/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 0.1698 - accuracy: 0.9648 - val_loss: 0.4509 - val_accuracy: 0.9381\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 0.1698 - accuracy: 0.9648 - val_loss: 0.4509 - val_accuracy: 0.9381\n",
            "Epoch 4/50\n",
            "Epoch 4/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 0.0924 - accuracy: 0.9801 - val_loss: 1.3456 - val_accuracy: 0.8704\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 0.0924 - accuracy: 0.9801 - val_loss: 1.3456 - val_accuracy: 0.8704\n",
            "Epoch 5/50\n",
            "Epoch 5/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 0.0997 - accuracy: 0.9831 - val_loss: 0.2515 - val_accuracy: 0.9558\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 0.0997 - accuracy: 0.9831 - val_loss: 0.2515 - val_accuracy: 0.9558\n",
            "Epoch 6/50\n",
            "Epoch 6/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 0.0247 - accuracy: 0.9938 - val_loss: 0.3306 - val_accuracy: 0.9573\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 0.0247 - accuracy: 0.9938 - val_loss: 0.3306 - val_accuracy: 0.9573\n",
            "Epoch 7/50\n",
            "Epoch 7/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.2428 - val_accuracy: 0.9647\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.2428 - val_accuracy: 0.9647\n",
            "Epoch 8/50\n",
            "Epoch 8/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 6.2346e-04 - accuracy: 0.9998 - val_loss: 0.2548 - val_accuracy: 0.9676\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 6.2346e-04 - accuracy: 0.9998 - val_loss: 0.2548 - val_accuracy: 0.9676\n",
            "Epoch 9/50\n",
            "Epoch 9/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 3.5659e-05 - accuracy: 1.0000 - val_loss: 0.2655 - val_accuracy: 0.9647\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 3.5659e-05 - accuracy: 1.0000 - val_loss: 0.2655 - val_accuracy: 0.9647\n",
            "Epoch 10/50\n",
            "Epoch 10/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 2.7452e-05 - accuracy: 1.0000 - val_loss: 0.2644 - val_accuracy: 0.9647\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 2.7452e-05 - accuracy: 1.0000 - val_loss: 0.2644 - val_accuracy: 0.9647\n",
            "Epoch 11/50\n",
            "Epoch 11/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 1.8953e-05 - accuracy: 1.0000 - val_loss: 0.2669 - val_accuracy: 0.9647\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 1.8953e-05 - accuracy: 1.0000 - val_loss: 0.2669 - val_accuracy: 0.9647\n",
            "Epoch 12/50\n",
            "Epoch 12/50\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 2.2819e-05 - accuracy: 1.0000 - val_loss: 0.2672 - val_accuracy: 0.9647\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 2.2819e-05 - accuracy: 1.0000 - val_loss: 0.2672 - val_accuracy: 0.9647\n",
            "Epoch 13/50\n",
            "Epoch 13/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 1.2831e-05 - accuracy: 1.0000 - val_loss: 0.2677 - val_accuracy: 0.9647\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 1.2831e-05 - accuracy: 1.0000 - val_loss: 0.2677 - val_accuracy: 0.9647\n",
            "Epoch 14/50\n",
            "Epoch 14/50\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 9.6741e-06 - accuracy: 1.0000 - val_loss: 0.2678 - val_accuracy: 0.9676\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 9.6741e-06 - accuracy: 1.0000 - val_loss: 0.2678 - val_accuracy: 0.9676\n",
            "Epoch 15/50\n",
            "Epoch 15/50\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 9.0402e-06 - accuracy: 1.0000 - val_loss: 0.2655 - val_accuracy: 0.9676\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 9.0402e-06 - accuracy: 1.0000 - val_loss: 0.2655 - val_accuracy: 0.9676\n",
            "Epoch 16/50\n",
            "Epoch 16/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 7.5365e-06 - accuracy: 1.0000 - val_loss: 0.2883 - val_accuracy: 0.9676\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 7.5365e-06 - accuracy: 1.0000 - val_loss: 0.2883 - val_accuracy: 0.9676\n",
            "Epoch 17/50\n",
            "Epoch 17/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 5.4396e-06 - accuracy: 1.0000 - val_loss: 0.2745 - val_accuracy: 0.9661\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 5.4396e-06 - accuracy: 1.0000 - val_loss: 0.2745 - val_accuracy: 0.9661\n",
            "Epoch 18/50\n",
            "Epoch 18/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 4.3318e-06 - accuracy: 1.0000 - val_loss: 0.2745 - val_accuracy: 0.9676\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 4.3318e-06 - accuracy: 1.0000 - val_loss: 0.2745 - val_accuracy: 0.9676\n",
            "Epoch 19/50\n",
            "Epoch 19/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 3.6137e-06 - accuracy: 1.0000 - val_loss: 0.2853 - val_accuracy: 0.9661\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 3.6137e-06 - accuracy: 1.0000 - val_loss: 0.2853 - val_accuracy: 0.9661\n",
            "Epoch 20/50\n",
            "Epoch 20/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 6.7879e-06 - accuracy: 1.0000 - val_loss: 0.2888 - val_accuracy: 0.9676\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 6.7879e-06 - accuracy: 1.0000 - val_loss: 0.2888 - val_accuracy: 0.9676\n",
            "Epoch 21/50\n",
            "Epoch 21/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 0.0348 - accuracy: 0.9945 - val_loss: 0.8726 - val_accuracy: 0.9588\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 0.0348 - accuracy: 0.9945 - val_loss: 0.8726 - val_accuracy: 0.9588\n",
            "Epoch 22/50\n",
            "Epoch 22/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.7973 - val_accuracy: 0.9529\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.7973 - val_accuracy: 0.9529\n",
            "Epoch 23/50\n",
            "Epoch 23/50\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 2.8020e-05 - accuracy: 1.0000 - val_loss: 0.7665 - val_accuracy: 0.9529\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 2.8020e-05 - accuracy: 1.0000 - val_loss: 0.7665 - val_accuracy: 0.9529\n",
            "Epoch 24/50\n",
            "Epoch 24/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 9.1684e-06 - accuracy: 1.0000 - val_loss: 0.7632 - val_accuracy: 0.9514\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 9.1684e-06 - accuracy: 1.0000 - val_loss: 0.7632 - val_accuracy: 0.9514\n",
            "Epoch 25/50\n",
            "Epoch 25/50\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 5.6148e-06 - accuracy: 1.0000 - val_loss: 0.7449 - val_accuracy: 0.9529\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 5.6148e-06 - accuracy: 1.0000 - val_loss: 0.7449 - val_accuracy: 0.9529\n",
            "Epoch 26/50\n",
            "Epoch 26/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 2.8025e-05 - accuracy: 1.0000 - val_loss: 0.7425 - val_accuracy: 0.9529\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 2.8025e-05 - accuracy: 1.0000 - val_loss: 0.7425 - val_accuracy: 0.9529\n",
            "Epoch 27/50\n",
            "Epoch 27/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 5.2482e-04 - accuracy: 0.9997 - val_loss: 0.6544 - val_accuracy: 0.9617\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 5.2482e-04 - accuracy: 0.9997 - val_loss: 0.6544 - val_accuracy: 0.9617\n",
            "Epoch 28/50\n",
            "Epoch 28/50\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 9.9869e-06 - accuracy: 1.0000 - val_loss: 0.6240 - val_accuracy: 0.9573\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 9.9869e-06 - accuracy: 1.0000 - val_loss: 0.6240 - val_accuracy: 0.9573\n",
            "Epoch 29/50\n",
            "Epoch 29/50\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 1.9793e-06 - accuracy: 1.0000 - val_loss: 0.6862 - val_accuracy: 0.9573\n",
            "596/596 [==============================] - 280s 469ms/step - loss: 1.9793e-06 - accuracy: 1.0000 - val_loss: 0.6862 - val_accuracy: 0.9573\n",
            "Epoch 30/50\n",
            "Epoch 30/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 1.5079e-06 - accuracy: 1.0000 - val_loss: 0.6634 - val_accuracy: 0.9573\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 1.5079e-06 - accuracy: 1.0000 - val_loss: 0.6634 - val_accuracy: 0.9573\n",
            "Epoch 31/50\n",
            "Epoch 31/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 4.3086e-07 - accuracy: 1.0000 - val_loss: 0.6347 - val_accuracy: 0.9588\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 4.3086e-07 - accuracy: 1.0000 - val_loss: 0.6347 - val_accuracy: 0.9588\n",
            "Epoch 32/50\n",
            "Epoch 32/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 3.1389e-07 - accuracy: 1.0000 - val_loss: 0.6646 - val_accuracy: 0.9573\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 3.1389e-07 - accuracy: 1.0000 - val_loss: 0.6646 - val_accuracy: 0.9573\n",
            "Epoch 33/50\n",
            "Epoch 33/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 2.7693e-07 - accuracy: 1.0000 - val_loss: 0.6499 - val_accuracy: 0.9558\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 2.7693e-07 - accuracy: 1.0000 - val_loss: 0.6499 - val_accuracy: 0.9558\n",
            "Epoch 34/50\n",
            "Epoch 34/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 1.7510e-07 - accuracy: 1.0000 - val_loss: 0.6782 - val_accuracy: 0.9573\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 1.7510e-07 - accuracy: 1.0000 - val_loss: 0.6782 - val_accuracy: 0.9573\n",
            "Epoch 35/50\n",
            "Epoch 35/50\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 2.4437e-07 - accuracy: 1.0000 - val_loss: 0.6555 - val_accuracy: 0.9573\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 2.4437e-07 - accuracy: 1.0000 - val_loss: 0.6555 - val_accuracy: 0.9573\n",
            "Epoch 36/50\n",
            "Epoch 36/50\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 1.7920e-07 - accuracy: 1.0000 - val_loss: 0.6679 - val_accuracy: 0.9573\n",
            "596/596 [==============================] - 280s 470ms/step - loss: 1.7920e-07 - accuracy: 1.0000 - val_loss: 0.6679 - val_accuracy: 0.9573\n",
            "Epoch 37/50\n",
            "Epoch 37/50\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 1.6679e-07 - accuracy: 1.0000 - val_loss: 0.6503 - val_accuracy: 0.9573\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 1.6679e-07 - accuracy: 1.0000 - val_loss: 0.6503 - val_accuracy: 0.9573\n",
            "Epoch 38/50\n",
            "Epoch 38/50\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 1.2517e-07 - accuracy: 1.0000 - val_loss: 0.6835 - val_accuracy: 0.9573\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 1.2517e-07 - accuracy: 1.0000 - val_loss: 0.6835 - val_accuracy: 0.9573\n",
            "Epoch 39/50\n",
            "Epoch 39/50\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 9.9681e-08 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 0.9573\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 9.9681e-08 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 0.9573\n",
            "Epoch 40/50\n",
            "Epoch 40/50\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 7.0485e-08 - accuracy: 1.0000 - val_loss: 0.6596 - val_accuracy: 0.9573\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 7.0485e-08 - accuracy: 1.0000 - val_loss: 0.6596 - val_accuracy: 0.9573\n",
            "Epoch 41/50\n",
            "Epoch 41/50\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 2.3613e-07 - accuracy: 1.0000 - val_loss: 0.6617 - val_accuracy: 0.9573\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 2.3613e-07 - accuracy: 1.0000 - val_loss: 0.6617 - val_accuracy: 0.9573\n",
            "Epoch 42/50\n",
            "Epoch 42/50\n",
            "596/596 [==============================] - 280s 471ms/step - loss: 3.8952e-08 - accuracy: 1.0000 - val_loss: 0.6510 - val_accuracy: 0.9558\n",
            "596/596 [==============================] - 280s 471ms/step - loss: 3.8952e-08 - accuracy: 1.0000 - val_loss: 0.6510 - val_accuracy: 0.9558\n",
            "Epoch 43/50\n",
            "Epoch 43/50\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 3.6146e-08 - accuracy: 1.0000 - val_loss: 0.6752 - val_accuracy: 0.9558\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 3.6146e-08 - accuracy: 1.0000 - val_loss: 0.6752 - val_accuracy: 0.9558\n",
            "Epoch 44/50\n",
            "Epoch 44/50\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 1.9588e-07 - accuracy: 1.0000 - val_loss: 0.6258 - val_accuracy: 0.9588\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 1.9588e-07 - accuracy: 1.0000 - val_loss: 0.6258 - val_accuracy: 0.9588\n",
            "Epoch 45/50\n",
            "Epoch 45/50\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 1.0558e-07 - accuracy: 1.0000 - val_loss: 0.6814 - val_accuracy: 0.9573\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 1.0558e-07 - accuracy: 1.0000 - val_loss: 0.6814 - val_accuracy: 0.9573\n",
            "Epoch 46/50\n",
            "Epoch 46/50\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 2.2201e-08 - accuracy: 1.0000 - val_loss: 0.6388 - val_accuracy: 0.9602\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 2.2201e-08 - accuracy: 1.0000 - val_loss: 0.6388 - val_accuracy: 0.9602\n",
            "Epoch 47/50\n",
            "Epoch 47/50\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 1.3279e-08 - accuracy: 1.0000 - val_loss: 0.6769 - val_accuracy: 0.9558\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 1.3279e-08 - accuracy: 1.0000 - val_loss: 0.6769 - val_accuracy: 0.9558\n",
            "Epoch 48/50\n",
            "Epoch 48/50\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 1.5767e-08 - accuracy: 1.0000 - val_loss: 0.6872 - val_accuracy: 0.9573\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 1.5767e-08 - accuracy: 1.0000 - val_loss: 0.6872 - val_accuracy: 0.9573\n",
            "Epoch 49/50\n",
            "Epoch 49/50\n",
            "596/596 [==============================] - 280s 471ms/step - loss: 9.0062e-09 - accuracy: 1.0000 - val_loss: 0.6430 - val_accuracy: 0.9558\n",
            "596/596 [==============================] - 280s 471ms/step - loss: 9.0062e-09 - accuracy: 1.0000 - val_loss: 0.6430 - val_accuracy: 0.9558\n",
            "Epoch 50/50\n",
            "Epoch 50/50\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 7.5894e-09 - accuracy: 1.0000 - val_loss: 0.6977 - val_accuracy: 0.9558\n",
            "596/596 [==============================] - 281s 471ms/step - loss: 7.5894e-09 - accuracy: 1.0000 - val_loss: 0.6977 - val_accuracy: 0.9558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5681"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5681"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW_cCm5qClpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history_last[fold]['loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm4CwPSqp7ru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0PuY2ltp9TB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index=str(index)\n",
        "type='final'\n",
        "model1='final'\n",
        "path='/content/gdrive/My Drive/'\n",
        "np.save(path+\"/best_accuracy_all_fold_\"+index+\"_\"+model1+\"_\"+type+\".npy\",best_accuracy_last)\n",
        "np.save(path+'/final_accuracy_all_fold'+index+\"_\"+model1+\"_\"+type+\".npy\",final_accuracy_last)\n",
        "np.save(path+'/history_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",history_last)\n",
        "np.save(path+'/answers_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",answers_last)\n",
        "np.save(path+'/predictions_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",predictions_last)\n",
        "np.save(path+'/predictions_all_best_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",predictions_last_best)\n",
        "np.save(path+'/times_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",times_last)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOdQm2sOWzCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}