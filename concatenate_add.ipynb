{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "concatenate_add.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMnbtwHtkQG9WQzpgau+C3A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/Multihead_attention/blob/master/concatenate_add.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thV1pKfhoLaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c5b911-c6d7-4afd-e4cc-a6d0412e4ee7"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "class LayerNormalization(Layer):\n",
        "    def __init__(self, eps=1e-6, **kwargs):\n",
        "        self.eps = eps\n",
        "        super(LayerNormalization, self).__init__(**kwargs)\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
        "                                     initializer=Ones(), trainable=True)\n",
        "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
        "                                    initializer=Zeros(), trainable=True)\n",
        "        super(LayerNormalization, self).build(input_shape)\n",
        "    def call(self, x):\n",
        "        mean = K.mean(x, axis=-1, keepdims=True)\n",
        "        std = K.std(x, axis=-1, keepdims=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "class abc(keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 head_num,\n",
        "                 q_k,\n",
        "                 activation='relu',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_normal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 history_only=False,\n",
        "                 **kwargs):\n",
        "        self.q_k=q_k\n",
        "        self.supports_masking = True\n",
        "        self.head_num = head_num\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
        "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
        "        self.kernel_constraint = keras.constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = keras.constraints.get(bias_constraint)\n",
        "        self.history_only = history_only\n",
        "\n",
        "        self.Wq = self.Wk = self.Wv = self.Wo = None\n",
        "        self.bq = self.bk = self.bv = self.bo = None\n",
        "\n",
        "        self.intensity = self.attention = None\n",
        "        super(abc, self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'head_num': self.head_num,\n",
        "            'activation': keras.activations.serialize(self.activation),\n",
        "            'use_bias': self.use_bias,\n",
        "            'kernel_initializer': keras.initializers.serialize(self.kernel_initializer),\n",
        "            'bias_initializer': keras.initializers.serialize(self.bias_initializer),\n",
        "            'kernel_regularizer': keras.regularizers.serialize(self.kernel_regularizer),\n",
        "            'bias_regularizer': keras.regularizers.serialize(self.bias_regularizer),\n",
        "            'kernel_constraint': keras.constraints.serialize(self.kernel_constraint),\n",
        "            'bias_constraint': keras.constraints.serialize(self.bias_constraint),\n",
        "            'history_only': self.history_only,\n",
        "        }\n",
        "        base_config = super(abc, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if isinstance(input_shape, list):\n",
        "            q, k, v = input_shape\n",
        "            return q[:-1] + (v[-1],)\n",
        "        return input_shape\n",
        "\n",
        "    def compute_mask(self, inputs, input_mask=None):\n",
        "        if isinstance(input_mask, list):\n",
        "            return input_mask[0]\n",
        "        return input_mask\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.layer_norm = LayerNormalization()\n",
        "        if isinstance(input_shape, list):\n",
        "            q, k, v = input_shape\n",
        "        else:\n",
        "            q = k = v = input_shape\n",
        "        feature_dim = int(v[-1])\n",
        "        if feature_dim % self.head_num != 0:\n",
        "            raise IndexError('Invalid head number %d with the given input dim %d' % (self.head_num, feature_dim))\n",
        "        self.Wq = self.add_weight(\n",
        "            shape=(int(q[-1]), self.q_k),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            name='%s_Wq' % self.name,\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.bq = self.add_weight(\n",
        "                shape=(self.q_k,),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name='%s_bq' % self.name,\n",
        "            )\n",
        "        self.Wk = self.add_weight(\n",
        "            shape=(int(k[-1]), self.q_k),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            name='%s_Wk' % self.name,\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.bk = self.add_weight(\n",
        "                shape=(self.q_k,),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name='%s_bk' % self.name,\n",
        "            )\n",
        "        self.Wv = self.add_weight(\n",
        "            shape=(int(v[-1]), feature_dim),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            name='%s_Wv' % self.name,\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.bv = self.add_weight(\n",
        "                shape=(feature_dim,),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name='%s_bv' % self.name,\n",
        "            )\n",
        "        self.Wo = self.add_weight(\n",
        "            shape=(feature_dim, feature_dim),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            name='%s_Wo' % self.name,\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.bo = self.add_weight(\n",
        "                shape=(feature_dim,),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name='%s_bo' % self.name,\n",
        "            )\n",
        "        super(abc, self).build(input_shape)\n",
        "\n",
        "    @staticmethod\n",
        "    def _reshape_to_batches(x, head_num):\n",
        "        input_shape = K.shape(x)\n",
        "        batch_size, seq_len, feature_dim = input_shape[0], input_shape[1], input_shape[2]\n",
        "        head_dim = feature_dim // head_num\n",
        "        x = K.reshape(x, (batch_size, seq_len, head_num, head_dim))\n",
        "        x = K.permute_dimensions(x, [0, 2, 1, 3])\n",
        "        return K.reshape(x, (batch_size * head_num, seq_len, head_dim))\n",
        "\n",
        "    @staticmethod\n",
        "    def _reshape_attention_from_batches(x, head_num):\n",
        "        input_shape = K.shape(x)\n",
        "        batch_size, seq_len, feature_dim = input_shape[0], input_shape[1], input_shape[2]\n",
        "        x = K.reshape(x, (batch_size // head_num, head_num, seq_len, feature_dim))\n",
        "        return K.permute_dimensions(x, [0, 2, 1, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def _reshape_from_batches(x, head_num):\n",
        "        input_shape = K.shape(x)\n",
        "        batch_size, seq_len, feature_dim = input_shape[0], input_shape[1], input_shape[2]\n",
        "        x = K.reshape(x, (batch_size // head_num, head_num, seq_len, feature_dim))\n",
        "        x = K.permute_dimensions(x, [0, 2, 1, 3])\n",
        "        return K.reshape(x, (batch_size // head_num, seq_len, feature_dim * head_num))\n",
        "\n",
        "    @staticmethod\n",
        "    def _reshape_mask(mask, head_num):\n",
        "        if mask is None:\n",
        "            return mask\n",
        "        seq_len = K.shape(mask)[1]\n",
        "        mask = K.expand_dims(mask, axis=1)\n",
        "        mask = K.tile(mask, [1, head_num, 1])\n",
        "        return K.reshape(mask, (-1, seq_len))\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if isinstance(inputs, list):\n",
        "            q, k, v = inputs\n",
        "        else:\n",
        "            q = k = v = inputs\n",
        "        if isinstance(mask, list):\n",
        "            q_mask, k_mask, v_mask = mask\n",
        "        else:\n",
        "            q_mask = k_mask = v_mask = mask\n",
        "        q = K.dot(q, self.Wq)\n",
        "        k = K.dot(k, self.Wk)\n",
        "        v = K.dot(v, self.Wv)\n",
        "        if self.use_bias:\n",
        "            q += self.bq\n",
        "            k += self.bk\n",
        "            v += self.bv\n",
        "        if self.activation is not None:\n",
        "            q = self.activation(q)\n",
        "            k = self.activation(k)\n",
        "            v = self.activation(v)\n",
        "        def scaled_dot_product_attention(inputs):\n",
        "          query, key, value = inputs\n",
        "          feature_dim = K.shape(query)[-1]\n",
        "          e = K.batch_dot(query, key, axes=2) / K.sqrt(K.cast(self.q_k, dtype=K.floatx()))\n",
        "          intensity = e\n",
        "          e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n",
        "          attention = e / K.sum(e, axis=-1, keepdims=True)\n",
        "          v = K.batch_dot(attention, value)\n",
        "          return v,intensity,attention\n",
        "       \n",
        "       \n",
        "        y,intensity,attention = scaled_dot_product_attention(\n",
        "            inputs=[\n",
        "                self._reshape_to_batches(q, self.head_num),\n",
        "                self._reshape_to_batches(k, self.head_num),\n",
        "                self._reshape_to_batches(v, self.head_num),\n",
        "            ]\n",
        "        )\n",
        "        self.intensity = self._reshape_attention_from_batches(intensity, self.head_num)\n",
        "        self.attention = self._reshape_attention_from_batches(attention, self.head_num)\n",
        "        y = self._reshape_from_batches(y, self.head_num)\n",
        "        y = K.dot(y, self.Wo)\n",
        "        if self.use_bias:\n",
        "            y += self.bo\n",
        "        if self.activation is not None:\n",
        "            y = self.activation(y)\n",
        "        return y\n",
        "from tensorflow import keras\n",
        "from keras.activations import softmax\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "class LayerNormalization(Layer):\n",
        "    def __init__(self, eps=1e-6, **kwargs):\n",
        "        self.eps = eps\n",
        "        super(LayerNormalization, self).__init__(**kwargs)\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
        "                                     initializer=Ones(), trainable=True)\n",
        "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
        "                                    initializer=Zeros(), trainable=True)\n",
        "        super(LayerNormalization, self).build(input_shape)\n",
        "    def call(self, x):\n",
        "        mean = K.mean(x, axis=-1, keepdims=True)\n",
        "        std = K.std(x, axis=-1, keepdims=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "class abc(Layer):\n",
        "    def __init__(self,inr,mo,up,**kwargs):\n",
        "        super(abc, self).__init__(**kwargs)\n",
        "        self.inr=inr\n",
        "        self.mo=mo\n",
        "        self.up=up\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super(abc, self).get_config()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(abc, self).build(input_shape)\n",
        "        self.cv1 = Conv2D(self.inr,1)\n",
        "        self.cv2 = Conv2D(self.inr,1)\n",
        "        self.cv3 = Conv2D(1,1)\n",
        "        self.up = UpSampling2D(interpolation='bilinear',size=(self.up,self.up))\n",
        "        self.dns1=Dense(1)\n",
        "    def call(self, img,y):\n",
        "        y = self.cv1(y)\n",
        "        x = self.cv2(img)\n",
        "        y = self.up(y)\n",
        "        \n",
        "        x = Add()([y,x])\n",
        "        x = ReLU()(x)\n",
        "        x = self.cv3(x)\n",
        "        \n",
        "        map = softmax(x,axis=[2,3])\n",
        "\n",
        "\n",
        "        return tf.math.multiply(img,map)\n",
        "\n",
        "class SpatialGate(keras.layers.Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(SpatialGate, self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super(SpatialGate, self).get_config()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(SpatialGate, self).build(input_shape)\n",
        "        self.cv2 = Conv2D(1,1)\n",
        "    def call(self, img):\n",
        "        \n",
        "        img_avg = K.expand_dims(K.mean(img,-1),-1)\n",
        "        img_max = K.expand_dims(K.max(img,-1),-1)\n",
        "        total = Concatenate(-1)([img_avg,img_max])\n",
        "        x = self.cv2(total)\n",
        "        x = keras.activations.sigmoid(x)\n",
        "\n",
        "        return tf.math.multiply(img,x)\n",
        "class ChannelGate(keras.layers.Layer):\n",
        "    def __init__(self,inr,ratio,**kwargs):\n",
        "        super(ChannelGate, self).__init__(**kwargs)\n",
        "        self.inr=inr\n",
        "        self.ratio=ratio\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super(abc, self).get_config()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ChannelGate, self).build(input_shape)\n",
        "        self.dns1 = Dense(self.inr/self.ratio,activation='relu')\n",
        "        self.dns2 = Dense(self.inr)\n",
        "        self.spt = SpatialGate()\n",
        "    def call(self, img):\n",
        "        \n",
        "        img_avg = self.dns2(self.dns1(GlobalAveragePooling2D()(img)))\n",
        "        img_max = self.dns2(self.dns1(GlobalMaxPooling2D()(img)))\n",
        "        x = keras.activations.sigmoid(img_max+img_avg)\n",
        "        x = Reshape((1,1,self.inr))(x)\n",
        "\n",
        "        return self.spt(tf.math.multiply(img,x))\n",
        "\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.applications import *\n",
        "def load_model():   \n",
        "  \n",
        "  K.clear_session() \n",
        "  mod=densenet.DenseNet121(include_top=True, weights='imagenet')\n",
        "  d = mod.get_layer('conv5_block16_concat').output\n",
        "  d = Conv2D(512,1)(d)\n",
        "  d = ChannelGate(512,8)(d)\n",
        "\n",
        "  d = GlobalAveragePooling2D()(d)\n",
        "\n",
        "  conc = Dense(3, activation=\"softmax\")(d) \n",
        "  mod=Model(inputs=mod.input,outputs=conc)\n",
        "  return mod\n",
        "\n",
        "\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.applications import *\n",
        "def load_model():   \n",
        "  \n",
        "  K.clear_session() \n",
        "  mod=densenet.DenseNet121(include_top=True, weights='imagenet')\n",
        "  d = mod.get_layer('conv5_block16_concat').output\n",
        "  d = ChannelGate(1024,8)(d)\n",
        "\n",
        "  a = mod.get_layer('conv3_block12_concat').output\n",
        "  a = ChannelGate(512,8)(a)\n",
        "\n",
        "  b = mod.get_layer('conv4_block24_concat').output\n",
        "  b = ChannelGate(1024,8)(b)\n",
        "    \n",
        "  a = GlobalAveragePooling2D()(a)\n",
        "\n",
        "  b = GlobalAveragePooling2D()(b)\n",
        "\n",
        "  d = GlobalAveragePooling2D()(d)\n",
        "\n",
        "  conc=Concatenate(axis=1)([a,b,d])\n",
        "  conc = Dense(3, activation=\"softmax\")(conc) \n",
        "  \n",
        "  mod=Model(inputs=mod.input,outputs=conc)\n",
        "  return mod"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 9185, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 9185 (delta 0), reused 0 (delta 0), pack-reused 9184\u001b[K\n",
            "Receiving objects: 100% (9185/9185), 9.60 MiB | 29.36 MiB/s, done.\n",
            "Resolving deltas: 100% (6383/6383), done.\n"
          ]
        }
      ]
    }
  ]
}