{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sesqueezenet_1_split.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/Multihead_attention/blob/master/sesqueezenet_1_split.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6RK3CfWAFFw",
        "outputId": "ef687ce5-f2a4-47ae-e4ed-c63aa786b1d8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osT-hzaTeNG0"
      },
      "source": [
        "pip install git+https://github.com/rcmalli/keras-squeezenet.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M98KUJbXJua9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuI9HKj_AIMs"
      },
      "source": [
        "import zipfile\n",
        "import h5py\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import glob, os\n",
        "from matplotlib import pyplot as plt\n",
        "import h5py\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import gc\n",
        "import pandas as pd\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "import zipfile\n",
        "import h5py\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import glob, os\n",
        "from matplotlib import pyplot as plt\n",
        "import h5py\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import gc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j37T69bkA217"
      },
      "source": [
        "path = F\"/content/gdrive/My Drive/check.npy\" \n",
        "df=np.load(path,allow_pickle=True)\n",
        "df=df.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2tyzHNZA3iX"
      },
      "source": [
        "\n",
        "#shuffle samples\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "\n",
        "\n",
        "#change targets\n",
        "def change(img):\n",
        "    resized = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA )\n",
        "    return resized\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#get train and test splits\n",
        "def get_trn_tst(df,tst_fold):\n",
        "  idx=np.asarray(df['fold'])\n",
        "  y=np.asarray(df['label'])\n",
        "  y-=1\n",
        "  img=np.asarray(df['image'])\n",
        "  img1=[]\n",
        "  for i in range(len(img)):\n",
        "        img1.append(change(img[i]))\n",
        "  img1=np.asarray(img1)\n",
        "  del([img])\n",
        "  gc.collect()\n",
        "  trn_y=np.asarray(y[(idx!=tst_fold)])\n",
        "  trn_img=np.asarray(img1[(idx!=tst_fold)])\n",
        "  tst_y=np.asarray(y[(idx==tst_fold)])\n",
        "  tst_img=img1[idx==tst_fold]\n",
        "  trn_img=np.repeat(trn_img.reshape((trn_img.shape[0],224,224,1)),3,axis=3)\n",
        "  tst_img=np.repeat(tst_img.reshape((tst_img.shape[0],224,224,1)),3,axis=3)\n",
        "  return (trn_img.copy(),trn_y.copy()),(tst_img.copy(),tst_y.copy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_qtwMUEA4QX"
      },
      "source": [
        "import zipfile\n",
        "import h5py\n",
        "import cv2\n",
        "import glob, os\n",
        "from matplotlib import pyplot as plt\n",
        "import h5py\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import gc\n",
        "import keras\n",
        "from keras.activations import softmax\n",
        "from keras import backend as K\n",
        "from keras.layers import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAkPnz9H0ab6"
      },
      "source": [
        "# from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Convolution2D, MaxPooling2D, Activation, concatenate, Dropout\n",
        "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D\n",
        "from keras.models import Model\n",
        "# from keras.engine.topology import get_source_inputs\n",
        "from tensorflow.keras.utils import get_file,get_source_inputs\n",
        "# from keras.utils import layer_utils\n",
        "\n",
        "\n",
        "sq1x1 = \"squeeze1x1\"\n",
        "exp1x1 = \"expand1x1\"\n",
        "exp3x3 = \"expand3x3\"\n",
        "relu1 = \"relu_\"\n",
        "\n",
        "WEIGHTS_PATH = \"https://github.com/rcmalli/keras-squeezenet/releases/download/v1.0/squeezenet_weights_tf_dim_ordering_tf_kernels.h5\"\n",
        "WEIGHTS_PATH_NO_TOP = \"https://github.com/rcmalli/keras-squeezenet/releases/download/v1.0/squeezenet_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "def squeeze_excite_block(input_tensor, ratio=16):\n",
        "    \"\"\" Create a channel-wise squeeze-excite block\n",
        "    Args:\n",
        "        input_tensor: input Keras tensor\n",
        "        ratio: number of output filters\n",
        "    Returns: a Keras tensor\n",
        "    References\n",
        "    -   [Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
        "    \"\"\"\n",
        "    init = input_tensor\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    filters = K.int_shape(init)[channel_axis]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation='relu', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', use_bias=False)(se)\n",
        "\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        se = Permute((3, 1, 2))(se)\n",
        "\n",
        "    x = multiply([init, se])\n",
        "    return x\n",
        "\n",
        "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
        "    s_id = 'fire' + str(fire_id) + '/'\n",
        "\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = 3\n",
        "    \n",
        "    x = Convolution2D(squeeze, (1, 1), padding='valid', name=s_id + sq1x1)(x)\n",
        "    x = Activation('relu', name=s_id + relu1 + sq1x1)(x)\n",
        "\n",
        "    left = Convolution2D(expand, (1, 1), padding='valid', name=s_id + exp1x1)(x)\n",
        "    left = Activation('relu', name=s_id + relu1 + exp1x1)(left)\n",
        "\n",
        "    right = Convolution2D(expand, (3, 3), padding='same', name=s_id + exp3x3)(x)\n",
        "    right = Activation('relu', name=s_id + relu1 + exp3x3)(right)\n",
        "\n",
        "    x = concatenate([left, right], axis=channel_axis, name=s_id + 'concat')\n",
        "    return x\n",
        "\n",
        "\n",
        "# Original SqueezeNet from paper.\n",
        "\n",
        "def SqueezeNet(include_top=True, weights='imagenet',\n",
        "               input_tensor=None, input_shape=(224,224,3),\n",
        "               pooling=None,\n",
        "               classes=1000):\n",
        "    \"\"\"Instantiates the SqueezeNet architecture.\n",
        "    \"\"\"\n",
        "        \n",
        "    if weights not in {'imagenet', None}:\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization) or `imagenet` '\n",
        "                         '(pre-training on ImageNet).')\n",
        "\n",
        "    if weights == 'imagenet' and classes != 1000:\n",
        "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
        "                         ' as true, `classes` should be 1000')\n",
        "\n",
        "\n",
        "    input_shape = input_shape\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "\n",
        "    x = Convolution2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv1')(img_input)\n",
        "    x = Activation('relu', name='relu_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
        "\n",
        "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
        "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
        "\n",
        "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
        "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
        "\n",
        "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
        "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
        "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
        "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
        "    \n",
        "    if include_top:\n",
        "        # It's not obvious where to cut the network... \n",
        "        # Could do the 8th or 9th layer... some work recommends cutting earlier layers.\n",
        "    \n",
        "        x = Dropout(0.5, name='drop9')(x)\n",
        "\n",
        "        x = Convolution2D(classes, (1, 1), padding='valid', name='conv10')(x)\n",
        "        x = Activation('relu', name='relu_conv10')(x)\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Activation('softmax', name='loss')(x)\n",
        "    else:\n",
        "        if pooling == 'avg':\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "        elif pooling=='max':\n",
        "            x = GlobalMaxPooling2D()(x)\n",
        "        elif pooling==None:\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"Unknown argument for 'pooling'=\" + pooling)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "\n",
        "    model = Model(inputs, x, name='squeezenet')\n",
        "\n",
        "    # load weights\n",
        "    if weights == 'imagenet':\n",
        "        if include_top:\n",
        "            weights_path = get_file('squeezenet_weights_tf_dim_ordering_tf_kernels.h5',\n",
        "                                    WEIGHTS_PATH,\n",
        "                                    cache_subdir='models')\n",
        "        else:\n",
        "            weights_path = get_file('squeezenet_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
        "                                    WEIGHTS_PATH_NO_TOP,\n",
        "                                    cache_subdir='models')\n",
        "            \n",
        "        model.load_weights(weights_path)\n",
        "        if K.backend() == 'theano':\n",
        "            layer_utils.convert_all_kernels_in_model(model)\n",
        "\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "\n",
        "            if K.backend() == 'tensorflow':\n",
        "                warnings.warn('You are using the TensorFlow backend, yet you '\n",
        "                              'are using the Theano '\n",
        "                              'image data format convention '\n",
        "                              '(`image_data_format=\"channels_first\"`). '\n",
        "                              'For best performance, set '\n",
        "                              '`image_data_format=\"channels_last\"` in '\n",
        "                              'your Keras config '\n",
        "                              'at ~/.keras/keras.json.')\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oLcgYmzA5BK"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras.optimizers import *\n",
        "    \n",
        "def Global_attention_block(inputs):\n",
        "    shape=K.int_shape(inputs)\n",
        "    x=Lambda(lambda x: K.mean(x,-1))(inputs)\n",
        "    x=Reshape((1,-1))(x)\n",
        "    x=softmax(x)# 1,HW\n",
        "    \n",
        "    y=Reshape((-1,shape[-1]))(inputs)#HW C\n",
        "    \n",
        "    x=K.batch_dot(x,y)#1 C\n",
        "    \n",
        "    p=GlobalAveragePooling2D() (inputs)\n",
        "    p=Reshape((1,1,shape[-1]))(p)#1 1 C\n",
        "    p=Conv2D(shape[-1],1,activation='relu')(p)\n",
        "    p=Conv2D(shape[-1],1,activation='softmax')(p)\n",
        "    p=Reshape((1,shape[-1]))(p)#1 C\n",
        "    \n",
        "    \n",
        "    x=Concatenate()([x,p])\n",
        "    x=Reshape((1,1,2*shape[-1]))(x)#1 1 C\n",
        "    x=Conv2D(shape[-1],1,activation='softmax')(x)\n",
        "    \n",
        "    return Multiply()([x,inputs])\n",
        "def load_model():   \n",
        "  K.clear_session() \n",
        "  mod1=SqueezeNet(include_top=True, weights='imagenet')\n",
        "  mod1.trainable=False\n",
        "  out_1=mod1.layers[-3].output\n",
        "    \n",
        "  p=Lambda(lambda x: x[:,:,:, :500])(out_1)\n",
        "  q=Lambda(lambda x: x[:,:,:, 500:])(out_1)\n",
        "    \n",
        "  p = Global_attention_block(p)\n",
        "  q = Global_attention_block(q)\n",
        "  \n",
        "  out_1=Concatenate()([p,q])\n",
        "\n",
        "  out_1 = GlobalAveragePooling2D()(out_1)\n",
        "  out=Dense(3,activation='softmax')(out_1)\n",
        "  model=Model(inputs=mod1.input,outputs=out)\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import time\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "\n",
        "\n",
        "#change targets\n",
        "def change(img):\n",
        "    resized = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA )\n",
        "    return resized\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "def rotate_image(image, angle):\n",
        "  image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "  result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
        "  return result\n",
        "def Hflip( images):\n",
        "\t\tseq = iaa.Sequential([iaa.Fliplr(1.0)])\n",
        "\t\treturn seq.augment_images(images)\n",
        "def Vflip( images):\n",
        "\t\tseq = iaa.Sequential([iaa.Flipud(1.0)])\n",
        "\t\treturn seq.augment_images(images)\n",
        "def noise(images):\n",
        "    ls=[]\n",
        "    for i in images:\n",
        "        x = np.random.normal(loc=0, scale=0.05, size=(299,299,3))\n",
        "        ls.append(i+x)\n",
        "    return ls\n",
        "def rotate(images):\n",
        "    ls=[]\n",
        "    for angle in range(-15,20,5):\n",
        "        for image in images:\n",
        "            ls.append(rotate_image(image,angle))\n",
        "    return ls\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "  def __init__(self, images, labels, batch_size=64, image_dimensions = (96 ,96 ,3), shuffle=False, augment=False):\n",
        "    self.labels       = labels              # array of labels\n",
        "    self.images = images        # array of image paths\n",
        "    self.batch_size   = batch_size          # batch size\n",
        "    self.on_epoch_end()\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.floor(self.labels.shape[0] / self.batch_size))\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.indexes = np.arange(self.labels.shape[0])\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\t\t# selects indices of data for next batch\n",
        "    indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "    # select data and load images\n",
        "    labels = self.labels.loc[indexes]\n",
        "    img = [self.images[k].astype(np.float32) for k in indexes]\n",
        "    imgH=Hflip(img)\n",
        "    imgV=Vflip(img)\n",
        "    imgR=rotate(img)\n",
        "    images=[]\n",
        "    images.extend(imgH)\n",
        "    images.extend(imgV)\n",
        "    images.extend(imgR)\n",
        "    lbl=labels.copy()\n",
        "    labels=pd.DataFrame()\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    #images = np.array([preprocess_input(img) for img in images])\n",
        "    return np.asarray(images), np.asarray(labels.values)"
      ],
      "metadata": {
        "id": "-qQYHsO_UyHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  best_accuracy_last={}\n",
        "  final_accuracy_last={}\n",
        "  history_last={}\n",
        "  answers_last={}\n",
        "  predictions_last={}\n",
        "  predictions_last_best={}\n",
        "  times_last={}"
      ],
      "metadata": {
        "id": "Bk7YRaAIUyjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53NNDYtlA8op"
      },
      "source": [
        "  def upd(dk,data):\n",
        "    if dk==0:\n",
        "        dk=data\n",
        "    else:\n",
        "        for ky in data.keys():\n",
        "            dk[ky].extend(data[ky])\n",
        "    return dk\n",
        "  index=1\n",
        "  epoch=50\n",
        "  pre_acc=0\n",
        "  best=0\n",
        "  fold='fold_'+str(index)\n",
        "  trn,tst=get_trn_tst(df,index)\n",
        "  history_last[fold]=0\n",
        "\n",
        "\n",
        "\n",
        "  plt.imshow(trn[0][0])\n",
        "  plt.show()\n",
        "  plt.imshow(tst[0][0])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  trn_x,trn_y=unison_shuffled_copies(trn[0],trn[1])\n",
        "  tst_x,tst_y=unison_shuffled_copies(tst[0],tst[1])\n",
        "\n",
        "\n",
        "\n",
        "  model=load_model()\n",
        "\n",
        "\n",
        "  \n",
        "  #compiling the model\n",
        "  train_data = DataGenerator(trn_x,pd.get_dummies(trn_y), batch_size=4, augment=True)\n",
        "  ln=len(trn_y)\n",
        "  # del([trn_x,trn_y,trn,tst])\n",
        "  # gc.collect()\n",
        "  #fitting the model\n",
        "  #timing\n",
        "  start=time.time()\n",
        "  print('training')\n",
        "  model.compile(optimizer=Adam(1e-2,decay=1e-3), \n",
        "                     loss='categorical_crossentropy', \n",
        "                     metrics=['accuracy'])\n",
        "  hist=model.fit_generator(train_data,epochs=1,verbose=1,steps_per_epoch=ln//4)\n",
        "\n",
        "  for i in model.layers:\n",
        "        i.trainable=True\n",
        "  model.compile(optimizer=Adam(2e-4,decay=1e-3), \n",
        "                     loss='categorical_crossentropy', \n",
        "                     metrics=['accuracy'])\n",
        "  hist=model.fit_generator(train_data,epochs=50,verbose=1,steps_per_epoch=ln//4)\n",
        "  history_last[fold]=upd(history_last[fold],hist.history)\n",
        "  del([train_data,trn_x,trn_y,trn,df])\n",
        "  gc.collect()\n",
        "  end=time.time()\n",
        "  times_last[fold]=end-start\n",
        "\n",
        "  #getting the prediction \n",
        "  pre=model.predict(tst_x)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  #select the maximum position\n",
        "  pre=np.argmax(pre,1)\n",
        "  predictions_last[fold]=pre\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  #getting the accuracy\n",
        "  new_acc=accuracy_score(pre,tst_y)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  #storing the predictions\n",
        "  final_accuracy_last[fold]=new_acc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #storing the answers\n",
        "  answers_last[fold]=tst_y\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  #freeing memory\n",
        "  del([tst_y,tst_x])\n",
        "  gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_last[fold]['loss'])"
      ],
      "metadata": {
        "id": "uGDGyeCLU_9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_acc"
      ],
      "metadata": {
        "id": "3lpaUSjBU_5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrY7XGfWkRDR"
      },
      "source": [
        "  print(new_acc)\n",
        "  index=str(index)\n",
        "  type='gated'\n",
        "  model1='squeezenet_'\n",
        "  path='/content/gdrive/My Drive/'\n",
        "  index=str(index)\n",
        "  np.save(path+\"/best_accuracy_all_fold_\"+index+\"_\"+model1+\"_\"+type+\".npy\",best_accuracy_last)\n",
        "  np.save(path+'/final_accuracy_all_fold'+index+\"_\"+model1+\"_\"+type+\".npy\",final_accuracy_last)\n",
        "  np.save(path+'/history_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",history_last)\n",
        "  np.save(path+'/answers_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",answers_last)\n",
        "  np.save(path+'/predictions_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",predictions_last)\n",
        "  np.save(path+'/predictions_all_best_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",predictions_last_best)\n",
        "  np.save(path+'/times_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",times_last)\n",
        "  model.save_weights(path+type+index+'.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wH3MEZF_5_jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JqJezzYk6Bd2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}