{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+nX3iNmkus/tKat1UHWYB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TapasKumarDutta1/Multihead_attention/blob/master/one_after_another_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo_asAC8q6iO"
      },
      "outputs": [],
      "source": [
        "def block(inputs,f):\n",
        "    a=Conv2D(f,3,padding='same')(inputs)\n",
        "    a=BatchNormalization()(a)\n",
        "    a=ReLU()(a)\n",
        "    b=Conv2D(f,3,padding='same')(a)\n",
        "    b=BatchNormalization()(b)\n",
        "    b=ReLU()(b)\n",
        "    c=Conv2D(f,3,padding='same')(b)\n",
        "    c=BatchNormalization()(c)\n",
        "    c=ReLU()(c)\n",
        "    d=Conv2D(f,3,padding='same')(c)\n",
        "    d=BatchNormalization()(d)\n",
        "    d=ReLU()(d)\n",
        "    mid=Concatenate()([a,b,c,d])\n",
        "    \n",
        "    x=Conv2D(f*4,1)(inputs)\n",
        "    x=BatchNormalization()(x)\n",
        "    x=ReLU()(x)\n",
        "    x=Add()([mid,x])\n",
        "    \n",
        "    y=Conv2D(f*2,1)(x)\n",
        "    y=BatchNormalization()(y)\n",
        "    y=ReLU()(y)\n",
        "    return y\n",
        "def Global_attention_block(C_A):\n",
        "#     shape=K.int_shape(inputs)\n",
        "#     x=AveragePooling2D(pool_size=(shape[1],shape[2])) (inputs)\n",
        "#     x=Conv2D(shape[3]//4,1, padding='same') (x)\n",
        "#     x=Activation('relu') (x)\n",
        "#     x=Conv2D(shape[3],1, padding='same') (x)\n",
        "#     x=Activation('sigmoid') (x)\n",
        "#     C_A=Multiply()([x,inputs])\n",
        "    \n",
        "    x=Lambda(lambda x: K.mean(x,axis=-1,keepdims=True))  (C_A)\n",
        "    x=Conv2D(1,1, padding='same') (x)\n",
        "    x=Activation('sigmoid') (x)\n",
        "    S_A=Multiply()([x,C_A])\n",
        "    return S_A\n",
        "\n",
        "def self_attention(inp):\n",
        "    shp=inp.shape\n",
        "    a=Conv2D(shp[-1]//8,1, padding='same') (inp)\n",
        "    a=Activation('sigmoid') (a)\n",
        "    b=Conv2D(shp[-1]//8,1, padding='same') (inp)\n",
        "    b=Activation('sigmoid') (b)\n",
        "    c=Conv2D(shp[-1]//8,1, padding='same') (inp)\n",
        "    c=Activation('sigmoid') (c)\n",
        "    \n",
        "    a=Reshape(( shp[1]*shp[2], shp[3]//8))(a)\n",
        "    b=Reshape(( shp[1]*shp[2], shp[3]//8))(b)\n",
        "    b=K.permute_dimensions(b, (0, 2, 1))\n",
        "    c=Reshape(( shp[1]*shp[2], shp[3]//8))(c)\n",
        "    inter=K.batch_dot(a,b)\n",
        "    out=K.batch_dot(inter,c)\n",
        "    out=Reshape(( shp[1],shp[2], shp[3]//8))(out)\n",
        "    out=Conv2D(shp[-1],1, padding='same') (out)\n",
        "    out=Reshape((shp[1],shp[2], shp[3]))(out)\n",
        "    \n",
        "    return out\n",
        "def load_model():   \n",
        "  K.clear_session() \n",
        "  inputs = Input(shape=(224,224,3))\n",
        "  x=Conv2D(16,3,padding='same')(inputs)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=ReLU()(x)\n",
        "  x=MaxPooling2D()(x)\n",
        "  \n",
        "  x=Conv2D(16,3,padding='same')(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=ReLU()(x)\n",
        "  x=MaxPooling2D()(x)\n",
        "    \n",
        "  a1=block(x,32)\n",
        "  x=MaxPooling2D()(a1)\n",
        "\n",
        "  a2=block(x,64)\n",
        "  x=MaxPooling2D()(a2)\n",
        "    \n",
        "  a3=block(x,128)\n",
        "  a3=self_attention(a3)\n",
        "  a3=Global_attention_block(a3)\n",
        "  x=GlobalAveragePooling2D()(a3)\n",
        "    \n",
        "  x=Dropout(0.5)(x)\n",
        "  x=Dense(3,activation='softmax')(x)\n",
        "  model = Model(inputs=inputs, outputs=x)\n",
        "  \n",
        "  return model\n",
        "\n"
      ]
    }
  ]
}