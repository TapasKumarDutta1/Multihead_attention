{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIBv/fKMk5kndFDvZTL7VW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TapasKumarDutta1/Multihead_attention/blob/master/c4_c2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4mh4RtMjzeF"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.activations import *\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.applications import *\n",
        "\n",
        "\n",
        "def self_attention(inp,st):\n",
        "    shp=inp.shape\n",
        "    a=Conv2D(shp[3]//8,1, padding='same',kernel_initializer='he_uniform') (inp)\n",
        "    a=Activation('relu') (a)\n",
        "    b=Conv2D(shp[3]//8,1, padding='same',kernel_initializer='he_uniform') (inp)\n",
        "    b=Activation('relu') (b)\n",
        "    c=Conv2D(shp[3]//8,1, padding='same',kernel_initializer='he_uniform') (inp)\n",
        "    c=Activation('relu') (c)\n",
        "    a=GlobalAveragePooling2D()(a)\n",
        "    a=Reshape(( 1, shp[3]//8),name='a'+st)(a)\n",
        "    b=Reshape(( shp[1]*shp[2], shp[3]//8),name='b'+st)(b)\n",
        "    b=K.permute_dimensions(b, (0, 2, 1))\n",
        "    c=Reshape(( shp[1]*shp[2], shp[3]//8),name='c'+st)(c)\n",
        "    inter=K.batch_dot(a,b)\n",
        "    inter=Activation('softmax') (inter)\n",
        "    out=K.batch_dot(inter,c)\n",
        "    out=Reshape(( 1,1, shp[3]//8),name='d'+st)(out)\n",
        "    out=Conv2D(shp[3],1, padding='same') (out)\n",
        "    out=Activation('sigmoid') (out)\n",
        "    out=out*inp\n",
        "    out=Reshape(( shp[1],shp[2], shp[3]),name='e'+st)(out)\n",
        "    \n",
        "    \n",
        "    inter=Reshape(( shp[1],shp[2], 1),name='f'+st)(inter)\n",
        "    out1=inter*inp\n",
        "    return Concatenate()([out,out1])\n",
        "\n",
        "def spatial_attention(C_A):\n",
        "    \n",
        "    x=Lambda(lambda x: K.max(x,axis=-1,keepdims=True))  (C_A)\n",
        "    x=Activation('relu') (x)\n",
        "    x=Conv2D(1,1, padding='same') (x)\n",
        "    x=Activation('sigmoid') (x)\n",
        "    S_A=Multiply()([x,C_A])\n",
        "    return S_A\n",
        "class MyLayer(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MyLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self._x = K.variable(0.5)\n",
        "        self._trainable_weights = [self._x]\n",
        "\n",
        "        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, x):\n",
        "        A, B = x\n",
        "        result = add([self._x*A ,(1-self._x)*B])\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0]\n",
        "    \n",
        "    \n",
        "    \n",
        "class GeM(Layer):\n",
        "    def __init__(self, init_norm=3.0, normalize=False, **kwargs):\n",
        "        self.init_norm = init_norm\n",
        "        self.normalize = normalize\n",
        "\n",
        "        super(GeM, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        feature_size = input_shape[-1]\n",
        "        self.p = self.add_weight(name=\"norms\", shape=(feature_size,),\n",
        "                                 initializer=keras.initializers.constant(self.init_norm),\n",
        "                                 trainable=True)\n",
        "        super(GeM, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        x = tf.math.maximum(x, 1e-6)\n",
        "        x = tf.pow(x, self.p)\n",
        "\n",
        "        \n",
        "        x=GlobalAveragePooling2D()(x)\n",
        "        x = tf.pow(x, 1.0 / self.p)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, input_shape[-1]])\n",
        "def gc_att(inp):\n",
        "    _,h,w,c=inp.shape\n",
        "    x=Conv2D(1,1,1)(inp)\n",
        "    x=Reshape((-1,1))(x)\n",
        "    x=Activation(\"softmax\")(x)#hw,1\n",
        "    y=Reshape((-1,h*w))(inp)#c, hw\n",
        "    x=K.batch_dot(y,x)\n",
        "    x=Reshape((1,1,-1))(x)#c, hw\n",
        "    x=Conv2D(c//8,1,1)(x)\n",
        "    x=LayerNormalization()(x)\n",
        "    x=Activation(\"relu\")(x)\n",
        "    x=Conv2D(c,1,1)(x)\n",
        "    return x+inp\n",
        "def load_model():   \n",
        "  K.clear_session() \n",
        "  mod=DenseNet121(include_top=True, weights='imagenet')\n",
        "  x = mod.layers[-3].output\n",
        "  p=self_attention(x,\"1\")\n",
        "  q=self_attention(x,\"2\")\n",
        "  r=self_attention(x,\"3\")\n",
        "  s=self_attention(x,\"4\")\n",
        "  x=Concatenate()([p,q,r,s])\n",
        "  x=Conv2D(1024,1,1)(x)\n",
        "  x=GeM()(x)\n",
        "  x=Dense(3,activation='softmax')(x)\n",
        "  model = Model(inputs=mod.input, outputs=x)\n",
        "  \n",
        "  return model"
      ]
    }
  ]
}