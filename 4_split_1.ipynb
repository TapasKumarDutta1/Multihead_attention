{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "4_split_1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/Multihead_attention/blob/master/4_split_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6RK3CfWAFFw",
        "outputId": "412e0723-321a-41b2-c1e4-93994194a708"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuI9HKj_AIMs"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import mixed_precision\n",
        "import zipfile\n",
        "import datetime, os\n",
        "import h5py\n",
        "from tensorflow.keras.optimizers import *\n",
        "import cv2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import glob, os\n",
        "from matplotlib import pyplot as plt\n",
        "import h5py\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import gc\n",
        "from tensorflow.keras.applications import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j37T69bkA217"
      },
      "source": [
        "path = F\"/content/gdrive/My Drive/check.npy\" \n",
        "df=np.load(path,allow_pickle=True)\n",
        "df=df.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2tyzHNZA3iX"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "class LayerNormalization(Layer):\n",
        "    def __init__(self, eps=1e-6, **kwargs):\n",
        "        self.eps = eps\n",
        "        super(LayerNormalization, self).__init__(**kwargs)\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
        "                                     initializer=Ones(), trainable=True)\n",
        "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
        "                                    initializer=Zeros(), trainable=True)\n",
        "        super(LayerNormalization, self).build(input_shape)\n",
        "    def call(self, x):\n",
        "        mean = K.mean(x, axis=-1, keepdims=True)\n",
        "        std = K.std(x, axis=-1, keepdims=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "def squeeze_excite_block(input_tensor, ratio=16):\n",
        "    \"\"\" Create a channel-wise squeeze-excite block\n",
        "    Args:\n",
        "        input_tensor: input Keras tensor\n",
        "        ratio: number of output filters\n",
        "    Returns: a Keras tensor\n",
        "    References\n",
        "    -   [Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
        "    \"\"\"\n",
        "    init = input_tensor\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    filters = K.int_shape(init)[channel_axis]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        se = Permute((3, 1, 2))(se)\n",
        "\n",
        "    x = multiply([init, se])\n",
        "    return x\n",
        "\n",
        "class abc(Layer):\n",
        "    def __init__(self,inr,size,mo,up,org,**kwargs):\n",
        "        super(abc, self).__init__(**kwargs)\n",
        "        self.inr=inr\n",
        "        self.mo=mo\n",
        "        self.up=up\n",
        "        self.org=org\n",
        "        self.size=size\n",
        "    def get_config(self):\n",
        "        base_config = super(abc, self).get_config()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(abc, self).build(input_shape)\n",
        "        self.cv1 = Conv2D(self.inr,1)\n",
        "        self.cv2 = Conv2D(self.inr,1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.dns1 = Conv2D(self.org,1,activation='relu')\n",
        "        self.dns2 = Conv2D(self.org,1,activation='sigmoid')\n",
        "        \n",
        "        \n",
        "        self.cv3 = Conv2D(1,1)\n",
        "        self.up = UpSampling2D(interpolation='bilinear',size=(self.up,self.up))\n",
        "        self.dns1=Dense(1)\n",
        "    def call(self, img,y):\n",
        "        y = self.cv1(y)\n",
        "        x = self.cv2(img)\n",
        "        y = self.up(y)\n",
        "        \n",
        "        y = Add()([y,x])\n",
        "        y=GlobalAveragePooling2D()(y)\n",
        "        y = Reshape((1,1,self.inr))(y)\n",
        "        x = self.dns1(y)\n",
        "        x = self.dns2(x)\n",
        "        z = tf.math.multiply(img,x)\n",
        "        \n",
        "        x = ReLU()(z)\n",
        "        x = K.max(x,axis=-1)\n",
        "        x = Reshape((self.size,self.size,1))(x)\n",
        "        \n",
        "        map = softmax(x,axis=[2,3])\n",
        "\n",
        "\n",
        "        return tf.math.multiply(z,map)\n",
        "\n",
        "class abc(Layer):\n",
        "    def __init__(self,inr,size,org,s,up,**kwargs):\n",
        "        super(abc, self).__init__(**kwargs)\n",
        "        self.inr=inr\n",
        "        self.up=up\n",
        "        self.org=org\n",
        "        self.size=size\n",
        "        self.s=s\n",
        "    def get_config(self):\n",
        "        base_config = super(abc, self).get_config()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(abc, self).build(input_shape)\n",
        "        self.cv2 = Conv2D(self.inr,1)\n",
        "        self.cv1 = Conv2D(self.inr,(self.s,self.s))\n",
        "        \n",
        "        \n",
        "        self.dns1 = Conv2D(self.org,1,activation='relu')\n",
        "        self.dns2 = Conv2D(self.org,1,activation='sigmoid')\n",
        "        \n",
        "        self.up = UpSampling2D(interpolation='bilinear',size=(self.up,self.up))\n",
        "    def call(self, img,y):\n",
        "        x = self.cv2(img)\n",
        "        y = self.up(y)\n",
        "        y = self.cv1(y)\n",
        "        y = Reshape((self.size,self.size,self.inr))(y)\n",
        "        x = Reshape((self.size,self.size,self.inr))(x)\n",
        "        y = Add()([y,x])\n",
        "        \n",
        "        \n",
        "        y=GlobalAveragePooling2D()(y)\n",
        "        y = Reshape((1,1,self.inr))(y)\n",
        "        x = self.dns1(y)\n",
        "        x = self.dns2(x)\n",
        "        z = tf.math.multiply(img,x)\n",
        "        \n",
        "        x = ReLU()(z)\n",
        "        x = K.max(x,axis=-1)\n",
        "        x = Reshape((self.size,self.size,1))(x)\n",
        "        \n",
        "        map = softmax(x,axis=[2,3])\n",
        "\n",
        "\n",
        "        return tf.math.multiply(z,map)\n",
        "\n",
        "class SpatialGate(keras.layers.Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(SpatialGate, self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super(SpatialGate, self).get_config()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(SpatialGate, self).build(input_shape)\n",
        "        self.cv2 = Conv2D(1,1)\n",
        "    def call(self, img):\n",
        "        \n",
        "        img_avg = K.expand_dims(K.mean(img,-1),-1)\n",
        "        img_max = K.expand_dims(K.max(img,-1),-1)\n",
        "        total = Concatenate(-1)([img_avg,img_max])\n",
        "        x = self.cv2(total)\n",
        "        x = keras.activations.sigmoid(x)\n",
        "\n",
        "        return tf.math.multiply(img,x)\n",
        "class ChannelGate(keras.layers.Layer):\n",
        "    def __init__(self,inr,ratio,**kwargs):\n",
        "        super(ChannelGate, self).__init__(**kwargs)\n",
        "        self.inr=inr\n",
        "        self.ratio=ratio\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super(abc, self).get_config()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ChannelGate, self).build(input_shape)\n",
        "        self.dns1 = Dense(self.inr/self.ratio,activation='relu')\n",
        "        self.dns2 = Dense(self.inr)\n",
        "        self.spt = SpatialGate()\n",
        "    def call(self, img):\n",
        "        \n",
        "        img_avg = self.dns2(self.dns1(GlobalAveragePooling2D()(img)))\n",
        "        img_max = self.dns2(self.dns1(GlobalMaxPooling2D()(img)))\n",
        "        x = keras.activations.sigmoid(img_max+img_avg)\n",
        "        x = Reshape((1,1,self.inr))(x)\n",
        "\n",
        "        return self.spt(tf.math.multiply(img,x))\n",
        "\n",
        "def Global_attention_block(inputs):\n",
        "    shape=K.int_shape(inputs)\n",
        "    x=MaxPooling2D(pool_size=(shape[1],shape[2])) (inputs)\n",
        "    x=Conv2D(shape[3],1, padding='same') (x)\n",
        "    x=Activation('relu') (x)\n",
        "    x=Conv2D(shape[3],1, padding='same') (x)\n",
        "    x=Activation('sigmoid') (x)\n",
        "    C_A=Multiply()([x,inputs])\n",
        "    \n",
        "    x=Lambda(lambda x: K.mean(x,axis=-1,keepdims=True))  (C_A)\n",
        "    x=Activation('sigmoid') (x)\n",
        "    S_A=Multiply()([x,C_A])\n",
        "    return S_A\n",
        "    \n",
        "    \n",
        "def Category_attention_block(inputs,classes,k):\n",
        "    shape=K.int_shape(inputs)\n",
        "    F=Conv2D(k*classes,1, padding='same') (inputs)\n",
        "    F=BatchNormalization() (F)\n",
        "    F1=Activation('relu') (F)\n",
        "    \n",
        "    F2=F1\n",
        "    x=GlobalAveragePooling2D()(F2)\n",
        "    \n",
        "    x=Reshape((classes,k)) (x)\n",
        "    \n",
        "    x=Reshape((shape[1],shape[2],classes,k)) (F1)\n",
        "    x=Lambda(lambda x: K.mean(x,axis=-1,keepdims=False))  (x)\n",
        "    x=Activation('sigmoid') (x)\n",
        "    x=Multiply()([S,x])\n",
        "    M=Lambda(lambda x: K.mean(x,axis=-1,keepdims=True))  (x)\n",
        "    M=Activation('sigmoid') (M)\n",
        "    \n",
        "    semantic=Multiply()([inputs,M])\n",
        "    return semantic\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.applications import *\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.applications import *\n",
        "    \n",
        "    \n",
        "def Category_attention_block(inputs,classes=3,k=5):\n",
        "    shape=K.int_shape(inputs)\n",
        "    F=Conv2D(k*classes,1, padding='same') (inputs)\n",
        "    F=BatchNormalization() (F)\n",
        "    F1=Activation('relu') (F)\n",
        "    \n",
        "    F2=F1\n",
        "    x=GlobalMaxPool2D()(F2)\n",
        "    \n",
        "    x=Reshape((classes,k)) (x)\n",
        "    S=Lambda(lambda x: K.mean(x,axis=-1,keepdims=False))  (x)\n",
        "    \n",
        "    x=Reshape((shape[1],shape[2],classes,k)) (F1)\n",
        "    x=Lambda(lambda x: K.mean(x,axis=-1,keepdims=False))  (x)\n",
        "    x=Multiply()([S,x])\n",
        "    M=Lambda(lambda x: K.mean(x,axis=-1,keepdims=True))  (x)\n",
        "    \n",
        "    semantic=Multiply()([inputs,M])\n",
        "    return semantic\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "def rotate_image(image, angle):\n",
        "  image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "  result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
        "  return result\n",
        "def Hflip( images):\n",
        "\t\tseq = iaa.Sequential([iaa.Fliplr(1.0)])\n",
        "\t\treturn seq.augment_images(images)\n",
        "def Vflip( images):\n",
        "\t\tseq = iaa.Sequential([iaa.Flipud(1.0)])\n",
        "\t\treturn seq.augment_images(images)\n",
        "def noise(images):\n",
        "    ls=[]\n",
        "    for i in images:\n",
        "        x = np.random.normal(loc=0, scale=0.05, size=(299,299,3))\n",
        "        ls.append(i+x)\n",
        "    return ls\n",
        "def rotate(images):\n",
        "    ls=[]\n",
        "    for angle in range(-15,20,5):\n",
        "        for image in images:\n",
        "            ls.append(rotate_image(image,angle))\n",
        "    return ls\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "  def __init__(self, images, labels, batch_size=64, image_dimensions = (96 ,96 ,3), shuffle=False, augment=False):\n",
        "    self.labels       = labels              # array of labels\n",
        "    self.images = images        # array of image paths\n",
        "    self.batch_size   = batch_size          # batch size\n",
        "    self.on_epoch_end()\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.floor(self.labels.shape[0] / self.batch_size))\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.indexes = np.arange(self.labels.shape[0])\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\t\t# selects indices of data for next batch\n",
        "    indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "    # select data and load images\n",
        "    labels = self.labels.loc[indexes]\n",
        "    img = [self.images[k].astype(np.float32) for k in indexes]\n",
        "    imgH=Hflip(img)\n",
        "    imgV=Vflip(img)\n",
        "    imgR=rotate(img)\n",
        "    images=[]\n",
        "    images.extend(imgH)\n",
        "    images.extend(imgV)\n",
        "    images.extend(imgR)\n",
        "    lbl=labels.copy()\n",
        "    labels=pd.DataFrame()\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    #images = np.array([preprocess_input(img) for img in images])\n",
        "    return np.asarray(images), labels.values\n",
        "class DataGenerator1(tf.keras.utils.Sequence):\n",
        "  def __init__(self, images, labels, batch_size=64, image_dimensions = (96 ,96 ,3), shuffle=False, augment=False):\n",
        "    self.labels       = labels              # array of labels\n",
        "    self.images = images        # array of image paths\n",
        "    self.batch_size   = batch_size          # batch size\n",
        "    self.on_epoch_end()\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.floor(self.labels.shape[0] / self.batch_size))\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.indexes = np.arange(self.labels.shape[0])\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\t\t# selects indices of data for next batch\n",
        "    indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "    # select data and load images\n",
        "    labels = self.labels.loc[indexes]\n",
        "    img = [self.images[k].astype(np.float32) for k in indexes]\n",
        "    imgH=Hflip(img)\n",
        "    imgV=Vflip(img)\n",
        "    imgR=rotate(img)\n",
        "    images=[]\n",
        "    images.extend(imgH)\n",
        "    images.extend(imgV)\n",
        "    images.extend(imgR)\n",
        "    lbl=labels.copy()\n",
        "    labels=pd.DataFrame()\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    #images = np.array([preprocess_input(img) for img in images])\n",
        "    return np.asarray(images), labels.values\n",
        "\n",
        "\n",
        "best_accuracy_last={}\n",
        "final_accuracy_last={}\n",
        "history_last={}\n",
        "answers_last={}\n",
        "predictions_last={}\n",
        "predictions_last_best={}\n",
        "times_last={}\n",
        "\n",
        "\n",
        "#shuffle samples\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "\n",
        "\n",
        "#change targets\n",
        "def change(img):\n",
        "    resized = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA )\n",
        "    return resized\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#get train and test splits\n",
        "def get_trn_tst(df,tst_fold):\n",
        "  idx=np.asarray(df['fold'])\n",
        "  y=np.asarray(df['label'])\n",
        "  y-=1\n",
        "  img=np.asarray(df['image'])\n",
        "  img1=[]\n",
        "  for i in range(len(img)):\n",
        "        img1.append(change(img[i]))\n",
        "  img1=np.asarray(img1)\n",
        "  del([img])\n",
        "  gc.collect()\n",
        "  trn_y=np.asarray(y[(idx!=tst_fold)])\n",
        "  trn_img=np.asarray(img1[(idx!=tst_fold)])\n",
        "  tst_y=np.asarray(y[(idx==tst_fold)])\n",
        "  tst_img=img1[idx==tst_fold]\n",
        "  trn_img=np.repeat(trn_img.reshape((trn_img.shape[0],224,224,1)),3,axis=3)\n",
        "  tst_img=np.repeat(tst_img.reshape((tst_img.shape[0],224,224,1)),3,axis=3)\n",
        "  return (trn_img.copy(),trn_y.copy()),(tst_img.copy(),tst_y.copy())\n",
        "\n",
        "def Global_attention_block(inputs):\n",
        "    shape=K.int_shape(inputs)\n",
        "    p0=Lambda(lambda x: x[:,:,:, :512])(inputs)\n",
        "    q0=Lambda(lambda x: x[:,:,:, 512:])(inputs)\n",
        "    \n",
        "    p=GlobalAveragePooling2D() (p0)\n",
        "    q=GlobalAveragePooling2D() (q0)\n",
        "    p=Reshape((1,1,512))(p)\n",
        "    q=Reshape((1,1,512))(q)\n",
        "    \n",
        "    p=Conv2D(256,1, padding='same') (p)\n",
        "    q=Conv2D(256,1, padding='same') (q)\n",
        "    p=BatchNormalization()(p)\n",
        "    q=BatchNormalization()(q)\n",
        "    q=Activation('relu')(q)\n",
        "    p=Activation('relu')(p)\n",
        "    \n",
        "    \n",
        "    p=Conv2D(512,1, padding='same',activation='sigmoid') (p)\n",
        "    q=Conv2D(512,1, padding='same',activation='sigmoid') (q)\n",
        "    \n",
        "    \n",
        "    p=Multiply()([p,p0])\n",
        "    q=Multiply()([q,q0])\n",
        "    return Concatenate()([p,q])\n",
        "from tensorflow.keras.layers import Activation,BatchNormalization\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "def Category_attention_block(inputs):\n",
        "    shape=K.int_shape(inputs)\n",
        "    k=Conv2D(512,3,padding='same') (inputs)\n",
        "    k=BatchNormalization()(k)\n",
        "    k=Activation('relu')(k)\n",
        "    k=Reshape([-1,512])(k)\n",
        "    k=K.transpose()(k)#C R\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    p1=Lambda(lambda x: x[:,:,:, :512])(inputs)\n",
        "    \n",
        "    pq1=Conv2D(512,3,padding='same') (p1)\n",
        "    pq1=BatchNormalization()(pq1)\n",
        "    pq1=Activation('relu')(pq1)\n",
        "    pq1=Reshape([-1,512])(pq1)#R C\n",
        "    \n",
        "    pv1=Conv2D(512,3,padding='same') (p1)\n",
        "    pv1=BatchNormalization()(pv1)\n",
        "    pv1=Activation('relu')(pv1)\n",
        "    pv1=Reshape([-1,512])(pv1)#R C\n",
        "    \n",
        "    q1=Lambda(lambda x: x[:,:,:, 512:])(inputs)\n",
        "    qq1=Conv2D(512,3,padding='same') (q1)\n",
        "    qq1=BatchNormalization()(qq1)\n",
        "    qq1=Activation('relu')(qq1)\n",
        "    qq1=Reshape([-1,512])(qq1)#R C\n",
        "    \n",
        "    qv1=Conv2D(512,3,padding='same') (q1)\n",
        "    qv1=BatchNormalization()(qv1)\n",
        "    qv1=Activation('relu')(qv1)\n",
        "    qv1=Reshape([-1,512])(qv1)#R C\n",
        "    \n",
        "    map_p=K.batch_dot()([k,pq1])#C C\n",
        "    map_p=sigmoid(map_p)\n",
        "    pv1=K.batch_dot()([pv1,map_p])#R C\n",
        "    pv1=Reshape([7,7,512])(pv1)#R C\n",
        "    \n",
        "    \n",
        "    map_q=K.batch_dot()([k,qq1])#C C\n",
        "    map_q=sigmoid(map_q)\n",
        "    qv1=K.batch_dot()([qv1,map_q])#R C\n",
        "    qv1=Reshape([7,7,512])(qv1)#R C\n",
        "    \n",
        "    return Concatenate()([pv1,qv1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_qtwMUEA4QX"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "from tensorflow.keras.optimizers import *\n",
        "    \n",
        "def Global_attention_block(inputs):\n",
        "    shape=K.int_shape(inputs)\n",
        "    x=Lambda(lambda x: K.mean(x,-1))(inputs)\n",
        "    x=Reshape((1,-1))(x)\n",
        "    x=sigmoid(x)# 1,HW\n",
        "    \n",
        "    y=Reshape((-1,shape[-1]))(inputs)#HW C\n",
        "    \n",
        "    x=K.batch_dot(x,y)#1 C\n",
        "    \n",
        "    p=GlobalAveragePooling2D() (inputs)\n",
        "    p=Reshape((1,1,shape[-1]))(p)#1 1 C\n",
        "    p=Conv2D(shape[-1],1,activation='relu')(p)\n",
        "    p=Conv2D(shape[-1],1,activation='sigmoid')(p)\n",
        "    p=Reshape((1,shape[-1]))(p)#1 C\n",
        "    \n",
        "    \n",
        "    x=Concatenate()([x,p])\n",
        "    x=Reshape((1,1,2*shape[-1]))(x)#1 1 C\n",
        "    x=Conv2D(shape[-1],1,activation='sigmoid')(x)\n",
        "    \n",
        "    return Multiply()([x,inputs])\n",
        "def load_model():   \n",
        "  K.clear_session() \n",
        "  mod1=DenseNet121(input_shape=(224,224,3))\n",
        "  mod1.trainable=False\n",
        "  out_1=mod1.layers[-3].output\n",
        "  p=Lambda(lambda x: x[:,:,:, :256])(out_1)\n",
        "  q=Lambda(lambda x: x[:,:,:, 256:512])(out_1)\n",
        "  s=Lambda(lambda x: x[:,:,:, 512:768])(out_1)\n",
        "  s=Lambda(lambda x: x[:,:,:, 768:])(out_1)\n",
        "    \n",
        "  p = Global_attention_block(p)\n",
        "  q = Global_attention_block(q)\n",
        "  r = Global_attention_block(r)\n",
        "  s = Global_attention_block(s)\n",
        "  \n",
        "  out_1=Concatenate()([p,q,r,s])\n",
        "  out_1 = GlobalMaxPooling2D()(out_1)\n",
        "  out=Dense(3,activation='softmax')(out_1)\n",
        "  model=Model(inputs=mod1.input,outputs=out)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0G0dTvqeVDF"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import time\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "\n",
        "\n",
        "#change targets\n",
        "def change(img):\n",
        "    resized = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA )\n",
        "    return resized\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "def rotate_image(image, angle):\n",
        "  image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "  result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
        "  return result\n",
        "def Hflip( images):\n",
        "\t\tseq = iaa.Sequential([iaa.Fliplr(1.0)])\n",
        "\t\treturn seq.augment_images(images)\n",
        "def Vflip( images):\n",
        "\t\tseq = iaa.Sequential([iaa.Flipud(1.0)])\n",
        "\t\treturn seq.augment_images(images)\n",
        "def noise(images):\n",
        "    ls=[]\n",
        "    for i in images:\n",
        "        x = np.random.normal(loc=0, scale=0.05, size=(299,299,3))\n",
        "        ls.append(i+x)\n",
        "    return ls\n",
        "def rotate(images):\n",
        "    ls=[]\n",
        "    for angle in range(-15,20,5):\n",
        "        for image in images:\n",
        "            ls.append(rotate_image(image,angle))\n",
        "    return ls\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "  def __init__(self, images, labels, batch_size=64, image_dimensions = (96 ,96 ,3), shuffle=False, augment=False):\n",
        "    self.labels       = labels              # array of labels\n",
        "    self.images = images        # array of image paths\n",
        "    self.batch_size   = batch_size          # batch size\n",
        "    self.on_epoch_end()\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.floor(self.labels.shape[0] / self.batch_size))\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.indexes = np.arange(self.labels.shape[0])\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\t\t# selects indices of data for next batch\n",
        "    indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "    # select data and load images\n",
        "    labels = self.labels.loc[indexes]\n",
        "    img = [self.images[k].astype(np.float32) for k in indexes]\n",
        "    imgH=Hflip(img)\n",
        "    imgV=Vflip(img)\n",
        "    imgR=rotate(img)\n",
        "    images=[]\n",
        "    images.extend(imgH)\n",
        "    images.extend(imgV)\n",
        "    images.extend(imgR)\n",
        "    lbl=labels.copy()\n",
        "    labels=pd.DataFrame()\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    #images = np.array([preprocess_input(img) for img in images])\n",
        "    return np.asarray(images), np.asarray(labels.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oLcgYmzA5BK"
      },
      "source": [
        "  best_accuracy_last={}\n",
        "  final_accuracy_last={}\n",
        "  history_last={}\n",
        "  answers_last={}\n",
        "  predictions_last={}\n",
        "  predictions_last_best={}\n",
        "  times_last={}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7Qxk4e8b3IQ"
      },
      "source": [
        "  def upd(dk,data):\n",
        "    if dk==0:\n",
        "        dk=data\n",
        "    else:\n",
        "        for ky in data.keys():\n",
        "            dk[ky].extend(data[ky])\n",
        "    return dk\n",
        "  index=1\n",
        "  epoch=50\n",
        "  pre_acc=0\n",
        "  best=0\n",
        "  fold='fold_'+str(index)\n",
        "  trn,tst=get_trn_tst(df,index)\n",
        "  history_last[fold]=0\n",
        "\n",
        "\n",
        "\n",
        "  plt.imshow(trn[0][0])\n",
        "  plt.show()\n",
        "  plt.imshow(tst[0][0])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  trn_x,trn_y=unison_shuffled_copies(trn[0],trn[1])\n",
        "  tst_x,tst_y=unison_shuffled_copies(tst[0],tst[1])\n",
        "\n",
        "\n",
        "\n",
        "  model=load_model()\n",
        "\n",
        "\n",
        "  \n",
        "  #compiling the model\n",
        "  train_data = DataGenerator(trn_x,pd.get_dummies(trn_y), batch_size=4, augment=True)\n",
        "  ln=len(trn_y)\n",
        "  # del([trn_x,trn_y,trn,tst])\n",
        "  # gc.collect()\n",
        "  #fitting the model\n",
        "  #timing\n",
        "  start=time.time()\n",
        "  print('training')\n",
        "  model.compile(optimizer=Adam(1e-2,decay=1e-3), \n",
        "                     loss='categorical_crossentropy', \n",
        "                     metrics=['accuracy'])\n",
        "  hist=model.fit_generator(train_data,epochs=1,verbose=1,steps_per_epoch=ln//4)\n",
        "\n",
        "  for i in model.layers:\n",
        "        i.trainable=True\n",
        "  model.compile(optimizer=Adam(2e-4,decay=1e-3), \n",
        "                     loss='categorical_crossentropy', \n",
        "                     metrics=['accuracy'])\n",
        "  hist=model.fit_generator(train_data,epochs=50,verbose=1,steps_per_epoch=ln//4)\n",
        "  history_last[fold]=upd(history_last[fold],hist.history)\n",
        "  del([train_data,trn_x,trn_y,trn,df])\n",
        "  gc.collect()\n",
        "  end=time.time()\n",
        "  times_last[fold]=end-start\n",
        "\n",
        "  #getting the prediction \n",
        "  pre=model.predict(tst_x)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  #select the maximum position\n",
        "  pre=np.argmax(pre,1)\n",
        "  predictions_last[fold]=pre\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  #getting the accuracy\n",
        "  new_acc=accuracy_score(pre,tst_y)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  #storing the predictions\n",
        "  final_accuracy_last[fold]=new_acc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #storing the answers\n",
        "  answers_last[fold]=tst_y\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  #freeing memory\n",
        "  del([tst_y,tst_x])\n",
        "  gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_last[fold]['loss'])"
      ],
      "metadata": {
        "id": "yr2BPGnvkO3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_acc"
      ],
      "metadata": {
        "id": "2FS-hHdkkO0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53NNDYtlA8op"
      },
      "source": [
        "  print(new_acc)\n",
        "  index=str(index)\n",
        "  type='gated'\n",
        "  model1='squeezenet_'\n",
        "  path='/content/gdrive/My Drive/'\n",
        "  index=str(index)\n",
        "  np.save(path+\"/best_accuracy_all_fold_\"+index+\"_\"+model1+\"_\"+type+\".npy\",best_accuracy_last)\n",
        "  np.save(path+'/final_accuracy_all_fold'+index+\"_\"+model1+\"_\"+type+\".npy\",final_accuracy_last)\n",
        "  np.save(path+'/history_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",history_last)\n",
        "  np.save(path+'/answers_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",answers_last)\n",
        "  np.save(path+'/predictions_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",predictions_last)\n",
        "  np.save(path+'/predictions_all_best_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",predictions_last_best)\n",
        "  np.save(path+'/times_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",times_last)\n",
        "  model.save_weights(path+type+index+'.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p5xVKTp4RmK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}